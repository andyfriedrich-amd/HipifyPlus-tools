#include "hip/hip_runtime.h"






#include <hipcub/hipcub.hpp>
#include <rocblas/rocblas.h>
#include <hip/hip_fp16.h>
#include <hip/hip_runtime.h>
#include <limits>

#include "core/providers/rocm/cu_inc/common.cuh"
#include "core/providers/rocm/rocm_common.h"
#include "contrib_ops/rocm/bert/add_bias_transpose.h"
#include "contrib_ops/rocm/bert/attention_impl.h"
#include "contrib_ops/rocm/bert/longformer_attention_softmax.h"
#include "contrib_ops/rocm/bert/longformer_attention_impl.h"

using namespace onnxruntime::rocm;
using namespace hipcub;

#define CHECK(expr) ROCBLAS_RETURN_IF_ERROR(expr)
#define CHECK_ROCM(expr) HIP_RETURN_IF_ERROR(expr)

namespace onnxruntime {
namespace contrib {
namespace rocm {















static size_t Align(size_t a) {
 const size_t alignment = 128; 
 return CeilDiv(a, alignment) * alignment;
}

size_t GetScratch1Size(size_t element_size, size_t batch_size, size_t num_heads, size_t sequence_length, size_t window) {
 size_t bytes = (5 * sequence_length - 3 * window) * window * num_heads * batch_size * element_size;
 return Align(bytes);
}

constexpr size_t GetScratch2Size() {
 return 5 * sizeof(void*) + 10 * sizeof(size_t);
}

size_t GetLongformerSoftmaxWorkspaceSize(
  size_t element_size, size_t batch_size, size_t num_heads, size_t sequence_length, size_t window, bool disable_compact_memory) {
 if (!disable_compact_memory) {
  size_t scratch1_size = GetScratch1Size(element_size, batch_size, num_heads, sequence_length, window);
  size_t scratch2_size = GetScratch2Size();
  return Align(scratch1_size + scratch2_size);
 } else {
  return 2 * GetAttentionScratchSize(element_size, batch_size, num_heads, sequence_length, sequence_length);
 }
}

size_t GetLongformerAttentionWorkspaceSize(
  size_t element_size, size_t batch_size, size_t num_heads, size_t head_size, size_t sequence_length, size_t max_num_global, size_t window, bool disable_compact_memory) {
 size_t softmax_size = GetLongformerSoftmaxWorkspaceSize(element_size, batch_size, num_heads, sequence_length, window, disable_compact_memory);
 size_t qkv_size = static_cast<size_t>(3) * batch_size * sequence_length * num_heads * head_size * element_size;
 size_t global_qkv_size = max_num_global > 0 ? qkv_size : 0;
 return softmax_size + qkv_size + global_qkv_size;
}



size_t GetPinnedBufferSize(size_t batch_size) {
 return sizeof(int) * batch_size + GetScratch2Size();
}


template <typename T, int blockSize>
__launch_bounds__(blockSize)
  __global__ void LongformerSoftmaxKernel(const int* global_attention, const int* global_index, const int* batch_global_num, void* buffer_pointers, const T* attention_mask, float scaler, int sequence_length, int num_heads, int window) {
 typedef hipcub::BlockReduce<float, blockSize> BlockReduce;
 __shared__ typename BlockReduce::TempStorage block_reduce_temp;

 int tid = threadIdx.x;
 const int batch_index = blockIdx.x / (sequence_length * num_heads);
 const int row_index = blockIdx.x % sequence_length;
 const int head_index = (blockIdx.x / sequence_length) % num_heads;

 
 const T* mask_block = attention_mask + sequence_length * batch_index;
 const int* global_index_block = global_index + sequence_length * batch_index;
 const int global_num = batch_global_num[batch_index];
 size_t* p_inputs = reinterpret_cast<size_t*>(buffer_pointers);
 size_t* p_outputs = reinterpret_cast<size_t*>(buffer_pointers);
 size_t* input_sizes = reinterpret_cast<size_t*>(buffer_pointers) + 5;
 size_t* input_strides = reinterpret_cast<size_t*>(buffer_pointers) + 10;
 const T* inputs[5];
 T* outputs[5];
 for (int i = 0; i < 5; ++i) {
  inputs[i] = reinterpret_cast<T*>(p_inputs[i]) + batch_index * num_heads * input_sizes[i];
  outputs[i] = reinterpret_cast<T*>(p_outputs[i]) + batch_index * num_heads * input_sizes[i];
 }

 
 int col_start = 0;
 int col_end = sequence_length;
 bool is_local_row = (global_attention[batch_index * sequence_length + row_index] == static_cast<int>(0));
 if (is_local_row) {
  col_start = row_index - window;
  if (col_start < 0) {
   col_start = 0;
  }

  col_end = row_index + window + 1;
  if (col_end > sequence_length) {
   col_end = sequence_length;
  }
 }

 
 if ((float)mask_block[row_index] != 0.f) {
  if (is_local_row) {
   T* output_block = nullptr;
   T* output_global = nullptr;
   int local_offset = row_index % window;
   int local_start = 0;
   int local_end = 3 * window;
   if (row_index < window) {
    local_start = 0;
    local_end = 2 * window;
    output_block = outputs[0] + row_index * input_strides[0] + head_index * input_sizes[0];
   } else if (row_index < sequence_length - window) {
    output_block = outputs[1] + (row_index - window) * input_strides[1] + head_index * input_sizes[1];
   } else {
    local_start = 0;
    local_end = 2 * window;
    output_block = outputs[2] + local_offset * input_strides[2] + head_index * input_sizes[2];
   }

   for (int i = local_start + tid; i < local_end; i += blockSize) {
    output_block[i] = 0;
   }

   if ((row_index - 2 * window) >= 0) {
    output_global = outputs[3] + (row_index - window) * input_strides[3] + head_index * input_sizes[3];
   }

   if (output_global != nullptr) {
    for (int i = tid; i < global_num; i += blockSize) {
     output_global[i] = 0;
    }
   }

  } else {
   T* output_block = outputs[4];
   for (int i = tid; i < sequence_length; i += blockSize)
    output_block[i] = 0;
  }
  return;
 }

 float sum_input = 0.;
 __shared__ float sum_shared;

 
 float max_input = -std::numeric_limits<float>::infinity();
 __shared__ float max_shared;

 if (is_local_row) {
  const T* input_block = nullptr;
  T* output_block = nullptr;
  T* output_global = nullptr;
  int local_offset = row_index % window;
  int local_start = local_offset;
  int local_end = local_start + 2 * window + 1;
  int zero_start = 0;
  int zero_end = 3 * window;
  if (row_index < window) {
   local_start = 0;
   local_end = local_offset + window + 1;
   zero_end = 2 * window;

   input_block = inputs[0] + row_index * input_strides[0] + head_index * input_sizes[0];
   output_block = outputs[0] + row_index * input_strides[0] + head_index * input_sizes[0];
  } else if (row_index < sequence_length - window) {
   input_block = inputs[1] + (row_index - window) * input_strides[1] + head_index * input_sizes[1];
   output_block = outputs[1] + (row_index - window) * input_strides[1] + head_index * input_sizes[1];
  } else {
   local_start = local_offset;
   local_end = 2 * window;
   zero_end = 2 * window;

   input_block = inputs[2] + local_offset * input_strides[2] + head_index * input_sizes[2];
   output_block = outputs[2] + local_offset * input_strides[2] + head_index * input_sizes[2];
  }

  const T* input_global = nullptr;
  int local_global = row_index - window;
  if (local_global > global_num) {
   local_global = global_num;
  }
  if (local_global > 0) {
   input_global = inputs[3] + (row_index - window) * input_strides[3] + head_index * input_sizes[3];
  }

  if (row_index < window) {
   output_global = (T*)outputs[0] + row_index * input_strides[0] + head_index * input_sizes[0];
  } else if (row_index < 2 * window) {
   output_global = outputs[1] + (row_index - window) * input_strides[1] + head_index * input_sizes[1];
  } else {
   output_global = outputs[3] + (row_index - window) * input_strides[3] + head_index * input_sizes[3];
  }

  for (int i = local_start + tid, j = col_start + tid; i < local_end; i += blockSize, j += blockSize) {
   float x = input_block[i];
   x = x * scaler + (float)mask_block[j];
   if (max_input < x)
    max_input = x;
  }

  if (input_global != nullptr) {
   for (int i = tid; i < local_global; i += blockSize) {
    float x = input_global[global_index_block[i]];
    x = x * scaler + (float)mask_block[global_index_block[i]];
    if (max_input < x)
     max_input = x;
   }
  }

  float max_block = BlockReduce(block_reduce_temp).Reduce(max_input, hipcub::Max());
  if (tid == 0) {
   max_shared = max_block;
  }
  __syncthreads();

  for (int i = local_start + tid, j = col_start + tid; i < local_end; i += blockSize, j += blockSize) {
   float x = input_block[i];
   x = expf((x)*scaler + (float)mask_block[j] - max_shared);
   sum_input += x;
  }

  if (input_global != nullptr) {
   for (int i = tid, j = col_start + tid; i < local_global; i += blockSize, j += blockSize) {
    float x = input_global[global_index_block[i]];
    x = expf((x)*scaler + (float)mask_block[j] - max_shared);
    sum_input += x;
   }
  }

  float sum_block = BlockReduce(block_reduce_temp).Reduce(sum_input, hipcub::Sum());
  if (tid == 0) {
   sum_shared = sum_block;
  }
  __syncthreads();
  float recip_sum = 1.f / sum_shared;

  for (int i = tid + zero_start; i < local_start; i += blockSize) {
   output_block[i] = (T)(0.);
  }

  for (int i = tid + local_end; i < zero_end; i += blockSize) {
   output_block[i] = (T)(0.);
  }

  __syncthreads();

  for (int i = local_start + tid, j = col_start + tid; i < local_end; i += blockSize, j += blockSize) {
   float x = input_block[i];
   x = expf((x)*scaler + (float)mask_block[j] - max_shared);
   output_block[i] = (T)(recip_sum * x);
  }

  if (input_global != nullptr) {
   for (int i = tid; i < local_global; i += blockSize) {
    float x = input_global[global_index_block[i]];
    x = expf((x)*scaler + (float)mask_block[global_index_block[i]] - max_shared);
    output_global[i] = (T)(recip_sum * x);
   }
  }
 } else {
  
  const T* input_block = inputs[4] + row_index * input_strides[4] + head_index * input_sizes[4];
  T* output_block = outputs[4] + row_index * input_strides[4] + head_index * input_sizes[4];

  for (int i = tid; i < sequence_length; i += blockSize) {
   float x = input_block[i];
   x = x * scaler + (float)mask_block[i];
   if (max_input < x)
    max_input = x;
  }

  float max_block = BlockReduce(block_reduce_temp).Reduce(max_input, hipcub::Max());
  if (tid == 0) {
   max_shared = max_block;
  }
  __syncthreads();

  for (int i = tid; i < sequence_length; i += blockSize) {
   float x = input_block[i];
   x = expf((x)*scaler + (float)mask_block[i] - max_shared);
   sum_input += x;
  }

  float sum_block = BlockReduce(block_reduce_temp).Reduce(sum_input, hipcub::Sum());
  if (tid == 0) {
   sum_shared = sum_block;
  }
  __syncthreads();
  float recip_sum = 1.f / sum_shared;

  for (int i = tid; i < sequence_length; i += blockSize) {
   float x = input_block[i];
   x = expf((x)*scaler + (float)mask_block[i] - max_shared);
   output_block[i] = (T)(recip_sum * x);
  }
 }
}

Status LaunchLongformerSoftmaxKernel(
  hipStream_t stream, rocblas_handle rocblas, void* workspace, const void* q, const void* k, const void* v, const void* attention_mask, int max_num_global, const bool compact_global_q, const void* global_q, const void* global_k, const void* global_v, const int* global_attention, const int* global_index, const int* batch_global_num, void* pinned_buffer, void* output, float scaler, int batch_size, int sequence_length, int num_heads, int head_size, int window, size_t element_size) {    
 const int* global_count = reinterpret_cast<const int*>(pinned_buffer);

 bool is_fp16 = (element_size == 2);
 char* scratch1 = reinterpret_cast<char*>(workspace);
 char* scratch2 = scratch1 + GetScratch1Size(element_size, batch_size, num_heads, sequence_length, window);

 
 rocblas_datatype_ Atype;
 rocblas_datatype_ Btype;
 rocblas_datatype_ Ctype;
 rocblas_datatype_ resultType;
 rocblas_gemm_algo algo = rocblas_gemm_algo_standard;

 __half one_fp16, zero_fp16;
 float one_fp32, zero_fp32;
 void *alpha, *beta_0, *beta_1;

 if (is_fp16) {
  one_fp16 = __float2half(1.f);
  zero_fp16 = __float2half(0.f);
  alpha = static_cast<void*>(&one_fp16);
  beta_0 = static_cast<void*>(&zero_fp16);
  beta_1 = static_cast<void*>(&one_fp16);
  Atype = rocblas_datatype_f16_r;
  Btype = rocblas_datatype_f16_r;
  Ctype = rocblas_datatype_f16_r;
  resultType = rocblas_datatype_f16_r;
  algo = rocblas_gemm_algo_standard;
 } else {
  one_fp32 = 1.f;
  zero_fp32 = 0.f;
  alpha = static_cast<void*>(&one_fp32);
  beta_0 = static_cast<void*>(&zero_fp32);
  beta_1 = static_cast<void*>(&one_fp32);
  Atype = rocblas_datatype_f32_r;
  Btype = rocblas_datatype_f32_r;
  Ctype = rocblas_datatype_f32_r;
  resultType = rocblas_datatype_f32_r;
 }

 
 
 
 
 size_t elements_per_batch = num_heads * sequence_length * head_size;
 int stride_per_head = sequence_length * head_size; 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 const int w = window;
 const int middle_count = (sequence_length - 2 * w) / w;
 int last_block = (sequence_length / w) - 1;

 

 
 size_t buffer_sizes[5] = {
   static_cast<size_t>(w * w * 2), static_cast<size_t>(w * w * middle_count * 3), static_cast<size_t>(w * w * 2), static_cast<size_t>(w * (sequence_length - w)), static_cast<size_t>(w * sequence_length)};    

 size_t buffer_strides[5] = {
   static_cast<size_t>(w * 2), static_cast<size_t>(w * 3), static_cast<size_t>(w * 2), static_cast<size_t>(w), static_cast<size_t>(sequence_length)};

 void* buffer_pointers[5];

 char* current_pointer = scratch1;
 for (int i = 0; i < 5; ++i) {
  buffer_pointers[i] = reinterpret_cast<void*>(current_pointer);
  current_pointer += buffer_sizes[i] * num_heads * batch_size * element_size;
 }

 

 char* temp_buffer = reinterpret_cast<char*>(pinned_buffer) + sizeof(int) * batch_size;
 memcpy(temp_buffer, &buffer_pointers[0], 5 * sizeof(void*));
 memcpy(temp_buffer + 5 * sizeof(void*), &buffer_sizes[0], 5 * sizeof(size_t));
 memcpy(temp_buffer + 5 * sizeof(void*) + 5 * sizeof(size_t), &buffer_strides[0], 5 * sizeof(size_t));
 CHECK_ROCM(hipMemcpyAsync(scratch2, temp_buffer, GetScratch2Size(), hipMemcpyHostToDevice, stream));

 
 {
  
  CHECK(_compat_rocblas_gemm_strided_batched_ex(rocblas, rocblas_operation_transpose, rocblas_operation_none, 2 * w, w, head_size, alpha, k, Atype, head_size, stride_per_head, q, Btype, head_size, stride_per_head, beta_0, buffer_pointers[0], Ctype, 2 * w, buffer_sizes[0], batch_size * num_heads, resultType, algo));

  
  if (middle_count > 0) {
   for (int i = 0; i < batch_size; ++i) {
    for (int j = 0; j < num_heads; ++j) {
     const void* q_head = reinterpret_cast<const char*>(q) +
                (i * elements_per_batch + (j * sequence_length + w) * head_size) * element_size;
     const void* k_head = reinterpret_cast<const char*>(k) +
                (i * elements_per_batch + j * sequence_length * head_size) * element_size;
     void* qk_head = reinterpret_cast<char*>(buffer_pointers[1]) +
             static_cast<size_t>(i * num_heads + j) * buffer_sizes[1] * element_size;
     CHECK(_compat_rocblas_gemm_strided_batched_ex(rocblas, rocblas_operation_transpose, rocblas_operation_none, 3 * w, w, head_size, alpha, k_head, Atype, head_size, w * head_size, q_head, Btype, head_size, w * head_size, beta_0, qk_head, Ctype, 3 * w, 3 * w * w, middle_count, resultType, algo));
    }
   }
  }

  
  const void* q_head = reinterpret_cast<const char*>(q) + (last_block * w * head_size) * element_size;
  const void* k_head = reinterpret_cast<const char*>(k) + ((last_block - 1) * w * head_size) * element_size;

  CHECK(_compat_rocblas_gemm_strided_batched_ex(rocblas, rocblas_operation_transpose, rocblas_operation_none, 2 * w, w, head_size, alpha, k_head, Atype, head_size, stride_per_head, q_head, Btype, head_size, stride_per_head, beta_0, buffer_pointers[2], Ctype, 2 * w, buffer_sizes[2], batch_size * num_heads, resultType, algo));
 }

 
 for (int i = 0; i < batch_size; ++i) {
  if (global_count[i] > 0) {
   const void* q_batch = reinterpret_cast<const char*>(q) + (i * elements_per_batch + w * head_size) * element_size;
   const void* k_batch = reinterpret_cast<const char*>(k) + (i * elements_per_batch) * element_size;
   void* qk_batch = reinterpret_cast<char*>(buffer_pointers[3]) + (i * buffer_sizes[3]) * num_heads * element_size;

   
   CHECK(_compat_rocblas_gemm_strided_batched_ex(rocblas, rocblas_operation_transpose, rocblas_operation_none, global_count[i], sequence_length - w, head_size, alpha, k_batch, Atype, head_size, stride_per_head, q_batch, Btype, head_size, stride_per_head, beta_0, qk_batch, Ctype, w, buffer_sizes[3], num_heads, resultType, algo));

   const size_t global_q_per_batch = compact_global_q ? num_heads * max_num_global * head_size : elements_per_batch;
   const int global_q_stride = (compact_global_q ? max_num_global * head_size : stride_per_head);
   const void* global_q_batch = reinterpret_cast<const char*>(global_q) + (i * global_q_per_batch) * element_size;
   const void* global_k_batch = reinterpret_cast<const char*>(global_k) + (i * elements_per_batch) * element_size;
   qk_batch = reinterpret_cast<char*>(buffer_pointers[4]) + (i * buffer_sizes[4] * num_heads) * element_size;

   
   
   CHECK(_compat_rocblas_gemm_strided_batched_ex(rocblas, rocblas_operation_transpose, rocblas_operation_none, sequence_length, global_count[i], head_size, alpha, global_k_batch, Atype, head_size, stride_per_head, global_q_batch, Btype, head_size, global_q_stride, beta_0, qk_batch, Ctype, sequence_length, buffer_sizes[4], num_heads, resultType, algo));
  }
 }

 const int blockSize = 64;
 const int gridSize = batch_size * num_heads * sequence_length;
 if (is_fp16) {
  LongformerSoftmaxKernel<__half, blockSize><<<gridSize, blockSize, 0, stream>>>(
    global_attention, global_index, batch_global_num, scratch2, static_cast<const __half*>(attention_mask), scaler, sequence_length, num_heads, window);
 } else {
  LongformerSoftmaxKernel<float, blockSize><<<gridSize, blockSize, 0, stream>>>(
    global_attention, global_index, batch_global_num, scratch2, static_cast<const float*>(attention_mask), scaler, sequence_length, num_heads, window);
 }

 
 {
  
  CHECK(_compat_rocblas_gemm_strided_batched_ex(rocblas, rocblas_operation_none, rocblas_operation_none, head_size, w, 2 * w, alpha, v, Atype, head_size, stride_per_head, buffer_pointers[0], Btype, static_cast<int>(buffer_strides[0]), buffer_sizes[0], beta_0, output, Ctype, head_size, stride_per_head, batch_size * num_heads, resultType, algo));

  
  if (middle_count > 0) {
   for (int i = 0; i < batch_size; ++i) {
    for (int j = 0; j < num_heads; ++j) {
     const void* v_head = reinterpret_cast<const char*>(v) +
                (i * elements_per_batch + j * head_size * sequence_length) * element_size;
     const void* prob_head = reinterpret_cast<const char*>(buffer_pointers[1]) +
                 (i * num_heads + j) * buffer_sizes[1] * element_size;
     void* out_head = reinterpret_cast<char*>(output) +
              (i * elements_per_batch + j * head_size * sequence_length + w * head_size) * element_size;
     CHECK(_compat_rocblas_gemm_strided_batched_ex(rocblas, rocblas_operation_none, rocblas_operation_none, head_size, w, 3 * w, alpha, v_head, Atype, head_size, w * head_size, prob_head, Btype, static_cast<int>(buffer_strides[1]), 3 * w * w, beta_0, out_head, Ctype, head_size, w * head_size, middle_count, resultType, algo));
    }
   }
  }

  
  const void* v_head = reinterpret_cast<const char*>(v) + (last_block - 1) * w * head_size * element_size;
  void* out_head = reinterpret_cast<char*>(output) + last_block * w * head_size * element_size;

  CHECK(_compat_rocblas_gemm_strided_batched_ex(rocblas, rocblas_operation_none, rocblas_operation_none, head_size, w, 2 * w, alpha, v_head, Atype, head_size, stride_per_head, buffer_pointers[2], Btype, static_cast<int>(buffer_strides[2]), buffer_sizes[2], beta_0, out_head, Ctype, head_size, stride_per_head, batch_size * num_heads, resultType, algo));
 }

 
 for (int i = 0; i < batch_size; ++i) {
  if (global_count[i] > 0) {
   
   const void* v_head = reinterpret_cast<const char*>(v) + (i * elements_per_batch) * element_size;
   const void* prob_head = reinterpret_cast<const char*>(buffer_pointers[3]) +
               (i * buffer_sizes[3] * num_heads + w * buffer_strides[3]) * element_size;
   void* out_head = reinterpret_cast<char*>(output) + (i * elements_per_batch + 2 * w * head_size) * element_size;

   CHECK(_compat_rocblas_gemm_strided_batched_ex(rocblas, rocblas_operation_none, rocblas_operation_none, head_size, sequence_length - 2 * w, global_count[i], alpha, v_head, Atype, head_size, stride_per_head, prob_head, Btype, static_cast<int>(buffer_strides[3]), buffer_sizes[3], beta_1, out_head, Ctype, head_size, stride_per_head, num_heads, resultType, algo));

   
   v_head = reinterpret_cast<const char*>(global_v) + (i * elements_per_batch) * element_size;
   prob_head = reinterpret_cast<const char*>(buffer_pointers[4]) + (i * buffer_sizes[4] * num_heads) * element_size;
   out_head = reinterpret_cast<char*>(output) + (i * elements_per_batch) * element_size;

   CHECK(_compat_rocblas_gemm_strided_batched_ex(rocblas, rocblas_operation_none, rocblas_operation_none, head_size, global_count[i], sequence_length, alpha, v_head, Atype, head_size, stride_per_head, prob_head, Btype, static_cast<int>(buffer_strides[4]), buffer_sizes[4], beta_0, out_head, Ctype, head_size, stride_per_head, num_heads, resultType, algo));
  }
 }

 return Status::OK();
}

template <typename T>
Status LongformerQkvToContext(
  const hipDeviceProp_t& device_prop, rocblas_handle rocblas, hipStream_t stream, const int batch_size, const int sequence_length, const int num_heads, const int head_size, const int window, const size_t element_size, const T* input, const T* bias, const T* attention_mask, const T* global_input, const T* global_bias, const int* global_attention, const int* global_index, const int* batch_global_num, const int max_num_global, void* pinned_buffer, T* workspace, T* output, size_t softmax_workspace_size, bool disable_compact_memory, bool use_merged_qkv_weights, bool use_half4) {
 T* qkv = reinterpret_cast<T*>(reinterpret_cast<char*>(workspace) + softmax_workspace_size);

 
 const int elements = batch_size * num_heads * sequence_length * head_size;

 const int max_threads_per_block(device_prop.maxThreadsPerBlock);

 const int format = static_cast<int>(use_merged_qkv_weights);
 bool compact_global_q = false;
 
 
 
 
 if (format == 1 || max_num_global == 0 || nullptr == global_input) {
  if (bias == nullptr) {
   ORT_RETURN_IF_ERROR(LaunchTransQkv(stream, 3, sequence_length, batch_size, head_size, num_heads, max_threads_per_block, false, input, qkv));
  } else {
   LaunchAddBiasTranspose(stream, 3, format, max_threads_per_block, batch_size, sequence_length, num_heads, head_size, input, bias, qkv, use_half4, head_size);
  }

  if (max_num_global > 0 && nullptr != global_input) {
   if (global_bias == nullptr) {
    ORT_RETURN_IF_ERROR(LaunchTransQkv(stream, 3, sequence_length, batch_size, head_size, num_heads, max_threads_per_block, false, global_input, qkv + 3 * elements));
   } else {
    LaunchAddBiasTranspose(stream, 3, format, max_threads_per_block, batch_size, sequence_length, num_heads, head_size, global_input, global_bias, qkv + 3 * elements, use_half4, head_size);
   }
  }
 } else {
  LaunchAddBiasTranspose(stream, 5, format, max_threads_per_block, batch_size, sequence_length, num_heads, head_size, input, bias, qkv, use_half4, head_size);

  compact_global_q = (disable_compact_memory == false);
  LaunchAddBiasTranspose(stream, 1, format, max_threads_per_block, batch_size, compact_global_q ? max_num_global : sequence_length, num_heads, head_size, global_input + 2 * elements, global_bias, qkv + 5 * elements, use_half4, head_size);
 }
 HIP_RETURN_IF_ERROR(hipGetLastError());

 
 const T* q = qkv;
 const T* k = q + elements;
 const T* v = k + elements;

 
 
 
 const T* global_q = (format == 1 ? v + elements : qkv + 5 * elements);
 const T* global_k = (format == 1 ? global_q + elements : qkv + 3 * elements);
 const T* global_v = (format == 1 ? global_k + elements : qkv + 4 * elements);

 
 const float rsqrt_head_size = 1.f / sqrt(static_cast<float>(head_size));

 T* temp_output = qkv; 

 if (disable_compact_memory) {
  ORT_RETURN_IF_ERROR(LaunchLongformerSoftmaxSimpleKernel(
      stream, rocblas, workspace, q, k, v, attention_mask, global_q, global_k, global_v, global_attention, global_index, batch_global_num, pinned_buffer, temp_output, rsqrt_head_size, batch_size, sequence_length, num_heads, head_size, window, element_size));
 } else {
  ORT_ENFORCE(max_num_global <= window);
  ORT_RETURN_IF_ERROR(LaunchLongformerSoftmaxKernel(
      stream, rocblas, workspace, q, k, v, attention_mask, max_num_global, compact_global_q, global_q, global_k, global_v, global_attention, global_index, batch_global_num, pinned_buffer, temp_output, rsqrt_head_size, batch_size, sequence_length, num_heads, head_size, window, element_size));
 }

 
 return LaunchTransCtx(stream, sequence_length, batch_size, head_size, num_heads, max_threads_per_block, false, temp_output, output);
}

Status LaunchLongformerAttentionKernel(
  const hipDeviceProp_t& device_prop, rocblas_handle rocblas, hipStream_t stream, const void* input, const void* bias, const void* attention_mask, const void* global_input, const void* global_bias, const int* global_attention, const int* global_index, const int* batch_global_num, void* pinned_buffer, void* workspace, void* output, int batch_size, int sequence_length, int num_heads, int head_size, int window, int max_num_global, const size_t element_size, bool disable_compact_memory, bool use_merged_qkv_weights, bool use_half4) {
 CompatRocblasMathModeSetter helper(device_prop, rocblas, 0 );
 size_t softmax_workspace_size = GetLongformerSoftmaxWorkspaceSize(element_size, batch_size, num_heads, sequence_length, window, disable_compact_memory);
 if (element_size == 2) {
  return LongformerQkvToContext(device_prop, rocblas, stream, batch_size, sequence_length, num_heads, head_size, window, element_size, reinterpret_cast<const half*>(input), reinterpret_cast<const half*>(bias), reinterpret_cast<const half*>(attention_mask), reinterpret_cast<const half*>(global_input), reinterpret_cast<const half*>(global_bias), global_attention, global_index, batch_global_num, max_num_global, pinned_buffer, reinterpret_cast<half*>(workspace), reinterpret_cast<half*>(output), softmax_workspace_size, disable_compact_memory, use_merged_qkv_weights, use_half4);
 } else {
  return LongformerQkvToContext(device_prop, rocblas, stream, batch_size, sequence_length, num_heads, head_size, window, element_size, reinterpret_cast<const float*>(input), reinterpret_cast<const float*>(bias), reinterpret_cast<const float*>(attention_mask), reinterpret_cast<const float*>(global_input), reinterpret_cast<const float*>(global_bias), global_attention, global_index, batch_global_num, max_num_global, pinned_buffer, reinterpret_cast<float*>(workspace), reinterpret_cast<float*>(output), softmax_workspace_size, disable_compact_memory, use_merged_qkv_weights, false);
 }
}

} 
} 
} 
