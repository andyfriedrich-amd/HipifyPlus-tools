


#include "instance_norm.h"
#include "instance_norm_impl.h"
#include "core/providers/cpu/nn/instance_norm_helper.h"
#include "core/providers/cpu/nn/batch_norm_helper.h"
#include "core/providers/cuda/math/unary_elementwise_ops_impl.h"

namespace onnxruntime {
namespace cuda {

#define REGISTER_KERNEL_TYPED(T)                   ONNX_OPERATOR_TYPED_KERNEL_EX(                     InstanceNormalization, kOnnxDomain, 6, T, kCudaExecutionProvider, (*KernelDefBuilder::Create())                     .TypeConstraint("T", DataTypeImpl::GetTensorType<T>()), InstanceNorm<T>);

REGISTER_KERNEL_TYPED(float)
REGISTER_KERNEL_TYPED(double)
REGISTER_KERNEL_TYPED(MLFloat16)

template <typename T>
InstanceNorm<T>::InstanceNorm(const OpKernelInfo& op_kernel_info)
  : CudaKernel(op_kernel_info) {
 float tmp_epsilon;
 ORT_ENFORCE(op_kernel_info.GetAttr<float>("epsilon", &tmp_epsilon).IsOK());
 epsilon_ = ClampCudnnBatchNormEpsilon(tmp_epsilon);
}

template <typename T>
Status InstanceNorm<T>::ComputeInternal(OpKernelContext* p_op_kernel_context) const {
 typedef typename ToCudaType<T>::MappedType CudaT;

 const Tensor* X = p_op_kernel_context->Input<Tensor>(0);
 const Tensor* scale = p_op_kernel_context->Input<Tensor>(1);
 const Tensor* bias = p_op_kernel_context->Input<Tensor>(2);

 ORT_RETURN_IF_ERROR(InstanceNormHelper::ValidateInputs(X, scale, bias));

 const TensorShape& x_shape = X->Shape();
 Tensor* Y = p_op_kernel_context->Output(0, x_shape);

 auto* y_data = reinterpret_cast<CudaT*>(Y->MutableData<T>());
 const auto* x_data = reinterpret_cast<const CudaT*>(X->Data<T>());
 const auto* scale_data = reinterpret_cast<const CudaT*>(scale->Data<T>());
 const auto* bias_data = reinterpret_cast<const CudaT*>(bias->Data<T>());

 const auto& x_dims = x_shape.GetDims();
 const int64_t N = x_dims[0];
 const int64_t C = x_dims[1];
 const auto one = Consts<CudaT>::One;
 const auto zero = Consts<CudaT>::Zero;

 if (N == 1) {
  
  

  CudnnTensor data_desc;
  std::vector<int64_t> new_dims;
  BatchNormHelper::NormalizeDims(x_shape, new_dims);
  ORT_RETURN_IF_ERROR(data_desc.Set(new_dims, CudnnTensor::GetDataType<CudaT>()));

  CudnnTensor stats_desc;
  ORT_RETURN_IF_ERROR(stats_desc.Set(data_desc, CUDNN_BATCHNORM_SPATIAL));

  CUDNN_RETURN_IF_ERROR(BatchNormalizationForwardTrainingHelper(
    GetCudnnHandle(p_op_kernel_context), CUDNN_BATCHNORM_SPATIAL, &one, &zero, data_desc, x_data, data_desc, y_data, stats_desc, scale_data, bias_data, 1.0f, nullptr, nullptr, epsilon_, nullptr, nullptr));
 } else {
  
  

  auto input_count = x_shape.Size();       
  auto stats_count = x_shape.SizeToDimension(2); 
  auto image_size = input_count / stats_count;

  CudnnTensor data_desc;
  ORT_RETURN_IF_ERROR(data_desc.Set(std::array<int64_t, 4>{1, stats_count, image_size, 1}, CudnnTensor::GetDataType<CudaT>()));

  CudnnTensor stats_desc;
  ORT_RETURN_IF_ERROR(stats_desc.Set(std::array<int64_t, 4>{1, stats_count, 1, 1}, CudnnTensor::GetDataType<CudaT>()));

  const size_t stats_byte_count = stats_count * sizeof(CudaT);

  
  auto mean = GetScratchBuffer<CudaT>(stats_count, p_op_kernel_context->GetComputeStream());
  CUDA_RETURN_IF_ERROR(cudaMemsetAsync(mean.get(), 0, stats_byte_count, Stream(p_op_kernel_context)));
  auto variance = GetScratchBuffer<CudaT>(stats_count, p_op_kernel_context->GetComputeStream());
  CUDA_RETURN_IF_ERROR(cudaMemsetAsync(variance.get(), 0, stats_byte_count, Stream(p_op_kernel_context)));

  
  auto unused_scale = GetScratchBuffer<CudaT>(stats_count, p_op_kernel_context->GetComputeStream());
  CUDA_RETURN_IF_ERROR(cudaMemsetAsync(unused_scale.get(), 0, stats_byte_count, Stream(p_op_kernel_context)));
  auto unused_bias = GetScratchBuffer<CudaT>(stats_count, p_op_kernel_context->GetComputeStream());
  CUDA_RETURN_IF_ERROR(cudaMemsetAsync(unused_bias.get(), 0, stats_byte_count, Stream(p_op_kernel_context)));

  
  CUDNN_RETURN_IF_ERROR(BatchNormalizationForwardTrainingHelper(
    GetCudnnHandle(p_op_kernel_context), CUDNN_BATCHNORM_SPATIAL, &one, &zero, data_desc, x_data, data_desc, y_data, stats_desc, unused_scale.get(), unused_bias.get(), 1.0f, mean.get(), variance.get(), CUDNN_BN_MIN_EPSILON, nullptr, nullptr));

  
  
  
  
  
  
  fast_divmod fdm_HW(gsl::narrow_cast<int>(image_size));
  fast_divmod fdm_C(gsl::narrow_cast<int>(C));

  InstanceNormImpl<CudaT>(
    Stream(p_op_kernel_context), x_data, scale_data, bias_data, mean.get(), variance.get(), (image_size - 1.0) / image_size, static_cast<double>(epsilon_), fdm_HW, fdm_C, y_data, input_count);
 }
 return Status::OK();
}

template <>
Status InstanceNorm<MLFloat16>::ComputeInternal(OpKernelContext* p_op_kernel_context) const {
 typedef typename ToCudaType<MLFloat16>::MappedType CudaT;

 const Tensor* X = p_op_kernel_context->Input<Tensor>(0);
 const Tensor* scale = p_op_kernel_context->Input<Tensor>(1);
 const Tensor* bias = p_op_kernel_context->Input<Tensor>(2);

 ORT_RETURN_IF_ERROR(InstanceNormHelper::ValidateInputs(X, scale, bias));

 const TensorShape& x_shape = X->Shape();
 Tensor* Y = p_op_kernel_context->Output(0, x_shape);

 auto* y_data = reinterpret_cast<CudaT*>(Y->MutableData<MLFloat16>());
 const auto* x_data = reinterpret_cast<const CudaT*>(X->Data<MLFloat16>());
 const auto* scale_data = reinterpret_cast<const CudaT*>(scale->Data<MLFloat16>());
 const auto* bias_data = reinterpret_cast<const CudaT*>(bias->Data<MLFloat16>());

 const auto& x_dims = x_shape.GetDims();
 const int64_t N = x_dims[0];
 const int64_t C = x_dims[1];
 const auto one = Consts<CudaT>::One;
 const auto zero = Consts<CudaT>::Zero;

 if (N == 1) {
  
  

  CudnnTensor data_desc;
  std::vector<int64_t> new_dims;
  BatchNormHelper::NormalizeDims(x_shape, new_dims);
  ORT_RETURN_IF_ERROR(data_desc.Set(new_dims, CudnnTensor::GetDataType<CudaT>()));

  CudnnTensor stats_desc;
  ORT_RETURN_IF_ERROR(stats_desc.Set(data_desc, CUDNN_BATCHNORM_SPATIAL));

  
  
  

  auto scale_data_fp32 = GetScratchBuffer<float>(C, p_op_kernel_context->GetComputeStream());
  Impl_Cast<CudaT, float>(Stream(p_op_kernel_context), scale_data, scale_data_fp32.get(), C);

  auto bias_data_fp32 = GetScratchBuffer<float>(C, p_op_kernel_context->GetComputeStream());
  Impl_Cast<CudaT, float>(Stream(p_op_kernel_context), bias_data, bias_data_fp32.get(), C);

  CUDNN_RETURN_IF_ERROR(BatchNormalizationForwardTrainingHelper(
    GetCudnnHandle(p_op_kernel_context), CUDNN_BATCHNORM_SPATIAL, &one, &zero, data_desc, x_data, data_desc, y_data, stats_desc, scale_data_fp32.get(), bias_data_fp32.get(), 1.0f, nullptr, nullptr, epsilon_, nullptr, nullptr));
 } else {
  
  

  auto input_count = x_shape.Size();       
  auto stats_count = x_shape.SizeToDimension(2); 
  auto image_size = input_count / stats_count;

  CudnnTensor data_desc;
  ORT_RETURN_IF_ERROR(data_desc.Set(std::array<int64_t, 4>{1, stats_count, image_size, 1}, CudnnTensor::GetDataType<CudaT>()));

  
  CudnnTensor stats_desc;
  ORT_RETURN_IF_ERROR(stats_desc.Set(std::array<int64_t, 4>{1, stats_count, 1, 1}, CudnnTensor::GetDataType<float>()));

  
  
  const size_t stats_byte_count = stats_count * sizeof(float);

  
  auto mean = GetScratchBuffer<float>(stats_count, p_op_kernel_context->GetComputeStream());
  CUDA_RETURN_IF_ERROR(cudaMemsetAsync(mean.get(), 0, stats_byte_count, Stream(p_op_kernel_context)));
  auto variance = GetScratchBuffer<float>(stats_count, p_op_kernel_context->GetComputeStream());
  CUDA_RETURN_IF_ERROR(cudaMemsetAsync(variance.get(), 0, stats_byte_count, Stream(p_op_kernel_context)));

  
  auto unused_scale = GetScratchBuffer<float>(stats_count, p_op_kernel_context->GetComputeStream());
  CUDA_RETURN_IF_ERROR(cudaMemsetAsync(unused_scale.get(), 0, stats_byte_count, Stream(p_op_kernel_context)));
  auto unused_bias = GetScratchBuffer<float>(stats_count, p_op_kernel_context->GetComputeStream());
  CUDA_RETURN_IF_ERROR(cudaMemsetAsync(unused_bias.get(), 0, stats_byte_count, Stream(p_op_kernel_context)));

  
  CUDNN_RETURN_IF_ERROR(BatchNormalizationForwardTrainingHelper(
    GetCudnnHandle(p_op_kernel_context), CUDNN_BATCHNORM_SPATIAL, &one, &zero, data_desc, x_data, data_desc, y_data, stats_desc, unused_scale.get(), unused_bias.get(), 1.0f, mean.get(), variance.get(), CUDNN_BN_MIN_EPSILON, nullptr, nullptr));

  
  
  
  
  
  
  fast_divmod fdm_HW(gsl::narrow_cast<int>(image_size));
  fast_divmod fdm_C(gsl::narrow_cast<int>(C));

  
  InstanceNormImpl<CudaT, float>(
    Stream(p_op_kernel_context), x_data, scale_data, bias_data, mean.get(), variance.get(), (image_size - 1.0) / image_size, static_cast<double>(epsilon_), fdm_HW, fdm_C, y_data, input_count);
 }

 return Status::OK();
}

} 
} 
