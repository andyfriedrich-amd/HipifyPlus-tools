#include "hip/hip_runtime.h"



#include "core/providers/rocm/cu_inc/common.cuh"
#include "core/providers/rocm/shared_inc/fast_divmod.h"

#include "cumsum_impl.h"

namespace onnxruntime {
namespace rocm {

template <typename T>
__global__ void _CumSumKernel(
  const T* input_data, const fast_divmod fast_divmod_input_dim_along_axis, const fast_divmod fast_divmod_input_stride_along_axis, T* output_data, const int64_t output_size, const bool exclusive, const bool reverse) {
 CALCULATE_ELEMENTWISE_INDEX_OR_EXIT(indices_index, output_size);

 int input_dim_along_axis = fast_divmod_input_dim_along_axis.d_;
 int input_stride_along_axis = fast_divmod_input_stride_along_axis.d_;

 int axis_dim = 0;
 int div = fast_divmod_input_stride_along_axis.div(static_cast<int>(indices_index));
 fast_divmod_input_dim_along_axis.divmod(div, div, axis_dim);

 int start = 0;
 int end = 0;

 if (!reverse && !exclusive) {
  start = 0;
  end = axis_dim;
 
 } else if (reverse && !exclusive) {
  start = axis_dim;
  end = input_dim_along_axis - 1;

 } else if (!reverse && exclusive) {
  start = 0;
  end = axis_dim - 1;

 } else { 
  start = axis_dim + 1;
  end = input_dim_along_axis - 1;

 }

 
 int count = end - start + 1;
 if (count <= 0) {
  output_data[indices_index] = 0;
  return; 
 }

 
 int data_index = static_cast<int>(indices_index) + (start - axis_dim) * input_stride_along_axis;
 T sum = 0;

 
 while (count != 0) {
  sum += input_data[data_index];
  data_index += input_stride_along_axis;
  --count;
 }

 output_data[indices_index] = sum;
}

template <typename T>
void CumSumImpl(
  hipStream_t stream, const T* input_data, const fast_divmod& input_dim_along_axis, const fast_divmod& input_stride_along_axis, T* output_data, int64_t output_size, bool exclusive, bool reverse) {
 if (output_size > 0) {
  int blocksPerGrid = static_cast<int>((output_size + GridDim::maxThreadsPerBlock - 1) / GridDim::maxThreadsPerBlock);

  _CumSumKernel<T><<<blocksPerGrid, GridDim::maxThreadsPerBlock, 0, stream>>>(input_data, input_dim_along_axis, input_stride_along_axis, output_data, output_size, exclusive, reverse);
 }
}

template void CumSumImpl<int32_t>(
  hipStream_t stream, const int32_t* input_data, const fast_divmod& input_dim_along_axis, const fast_divmod& input_stride_along_axis, int32_t* output_data, int64_t output_size, bool exclusive, bool reverse);

template void CumSumImpl<int64_t>(
  hipStream_t stream, const int64_t* input_data, const fast_divmod& input_dim_along_axis, const fast_divmod& input_stride_along_axis, int64_t* output_data, int64_t output_size, bool exclusive, bool reverse);

template void CumSumImpl<uint32_t>(
  hipStream_t stream, const uint32_t* input_data, const fast_divmod& input_dim_along_axis, const fast_divmod& input_stride_along_axis, uint32_t* output_data, int64_t output_size, bool exclusive, bool reverse);

template void CumSumImpl<uint64_t>(
  hipStream_t stream, const uint64_t* input_data, const fast_divmod& input_dim_along_axis, const fast_divmod& input_stride_along_axis, uint64_t* output_data, int64_t output_size, bool exclusive, bool reverse);

template void CumSumImpl<float>(
  hipStream_t stream, const float* input_data, const fast_divmod& input_dim_along_axis, const fast_divmod& input_stride_along_axis, float* output_data, int64_t output_size, bool exclusive, bool reverse);

template void CumSumImpl<double>(
  hipStream_t stream, const double* input_data, const fast_divmod& input_dim_along_axis, const fast_divmod& input_stride_along_axis, double* output_data, int64_t output_size, bool exclusive, bool reverse);

template void CumSumImpl<half>(
  hipStream_t stream, const half* input_data, const fast_divmod& input_dim_along_axis, const fast_divmod& input_stride_along_axis, half* output_data, int64_t output_size, bool exclusive, bool reverse);

} 
} 

