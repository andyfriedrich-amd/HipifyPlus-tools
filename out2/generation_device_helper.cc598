


#include <utility>
#include <memory>
#include "core/providers/shared_library/provider_api.h"
#include "core/providers/cuda/cuda_kernel.h"
#include "core/providers/cuda/math/topk_impl.h"
#include "core/providers/cuda/math/softmax.h"
#include "core/providers/cuda/shared_inc/accumulation_type.h"
#include "core/framework/ort_value.h"
#include "contrib_ops/cuda/bert/transformer_cuda_common.h"
#include <cuda_runtime.h>
#include "contrib_ops/cuda/transformers/generation_cuda_impl.h"
#include "contrib_ops/cuda/transformers/dump_cuda_tensor.h"
#include "contrib_ops/cpu/transformers/subgraph_t5_decoder.h"
#include "contrib_ops/cpu/transformers/subgraph_gpt.h"
#include "contrib_ops/cuda/transformers/beam_search_topk.h"
#include "contrib_ops/cuda/transformers/greedy_search_top_one.h"
#include "core/providers/cuda/tensor/transpose.h"


#ifdef ENABLE_NVTX_PROFILE
#include "core/providers/cuda/nvtx_profile.h"
#include "core/providers/cuda/nvtx_profile_context.h"
#endif

#include "sampling_cuda_helper.h"

#ifdef DEBUG_GENERATION
#include <iostream>
#endif

using onnxruntime::cuda::TArray;
using onnxruntime::cuda::ToCudaType;
using onnxruntime::cuda::TopKImpl;

namespace onnxruntime {
namespace concurrency {
class ThreadPool;
}
} 

#include "generation_device_helper.h"

namespace onnxruntime {
namespace contrib {
namespace GenerationCudaDeviceHelper {









Status ReorderPastState(
  const void* cuda_device_prop, Tensor& past_state, Tensor& past_state_staging, Stream* stream) {
 ORT_ENFORCE(stream);
 cudaStream_t cuda_stream = reinterpret_cast<cudaStream_t>(stream->GetHandle());
 cublasHandle_t cublas_handle = static_cast<CudaStream*>(stream)->cublas_handle_;

 const auto& past_state_shape = past_state.Shape();

 const auto& past_state_dims = past_state_shape.GetDims();
 const bool packed_past = past_state_dims.size() == 5;

 
 size_t past_state_size = packed_past ? past_state.SizeInBytes() / 2 : past_state.SizeInBytes();
 void* past_state_staging_buffer = past_state_staging.MutableDataRaw();
 CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(past_state_staging_buffer, past_state.DataRaw(), past_state_size, cudaMemcpyDeviceToDevice, cuda_stream));

 
 
 int64_t chunk_size = static_cast<int64_t>(16 / past_state.DataType()->Size());

 std::vector<size_t> permutation_vector = {0, 1, 3, 2, 4};
 gsl::span<size_t> permutation(permutation_vector.data(), 5);

 
 size_t offset = packed_past ? 1 : 0;
 TensorShape transpose_input_shape_override = {past_state_shape[offset], past_state_shape[offset + 1], past_state_shape[offset + 2], past_state_shape[offset + 3] / chunk_size, chunk_size};

 TensorShape transpose_output_shape_override = {past_state_shape[offset], past_state_shape[offset + 1], past_state_shape[offset + 3] / chunk_size, past_state_shape[offset + 2], chunk_size};

 
 return onnxruntime::cuda::Transpose::DoTranspose(*static_cast<const cudaDeviceProp*>(cuda_device_prop), cuda_stream, cublas_handle, permutation, past_state_staging, past_state, &transpose_input_shape_override, &transpose_output_shape_override);
}

Status InitCacheIndir(Tensor& cache_indir, Stream* stream) {
 ORT_ENFORCE(stream);
 cudaStream_t cuda_stream = reinterpret_cast<cudaStream_t>(stream->GetHandle());

 
 CUDA_RETURN_IF_ERROR(cudaMemsetAsync(cache_indir.MutableDataRaw(), 0, cache_indir.SizeInBytes(), cuda_stream));

 return Status::OK();
}

Status TopK(const Tensor* input, const int axis, const unsigned k, bool largest, bool sorted, AllocatorPtr allocator, Stream* stream, onnxruntime::concurrency::ThreadPool* , Tensor& output_values, Tensor& output_indices) {
#ifdef ENABLE_NVTX_PROFILE
 profile::NvtxNestedRangeCreator topkRange("TopK", profile::Color::Green);
 topkRange.Begin();
#endif

 ORT_ENFORCE(nullptr != input);
 int32_t rank = static_cast<int32_t>(input->Shape().NumDimensions());

 ORT_ENFORCE(axis >= 0 && axis < rank);
 ORT_ENFORCE(k > 0 && k <= input->Shape().GetDims()[axis]);

 auto input_shape = input->Shape();
 auto output_shape = input_shape;
 output_shape[axis] = k;

 TArray<int64_t> elem_nums_cuda(input->Shape().GetDims());
 for (int32_t i = elem_nums_cuda.Size() - 2; i >= 0; --i) {
  elem_nums_cuda[i] *= elem_nums_cuda[i + 1];
 }

 int64_t dimension = input_shape[axis];
 int64_t N = elem_nums_cuda[0] / dimension;

 output_values = std::move(*Tensor::Create(input->DataType(), output_shape, allocator));
 output_indices = std::move(*Tensor::Create(DataTypeImpl::GetType<int64_t>(), output_shape, std::move(allocator)));

 Status result;
 if (input->IsDataType<float>()) {
  result = TopKImpl<float>(nullptr, stream, input->Data<float>(), static_cast<float*>(output_values.MutableDataRaw()), static_cast<int64_t*>(output_indices.MutableDataRaw()), elem_nums_cuda, static_cast<size_t>(elem_nums_cuda.Size()), static_cast<int32_t>(axis), static_cast<int64_t>(k), static_cast<int64_t>(largest), static_cast<int64_t>(sorted), N, dimension);
 } else if (input->IsDataType<MLFloat16>()) {
  result = TopKImpl<MLFloat16>(nullptr, stream, input->Data<MLFloat16>(), static_cast<MLFloat16*>(output_values.MutableDataRaw()), static_cast<int64_t*>(output_indices.MutableDataRaw()), elem_nums_cuda, static_cast<size_t>(elem_nums_cuda.Size()), static_cast<int32_t>(axis), static_cast<int64_t>(k), static_cast<int64_t>(largest), static_cast<int64_t>(sorted), N, dimension);
 } else {
  result = ORT_MAKE_STATUS(ONNXRUNTIME, NOT_IMPLEMENTED, "BeamSearch op: An implementation for the input type ", input->DataType(), " is not supported yet");
 }

#ifdef ENABLE_NVTX_PROFILE
 topkRange.End();
#endif
 return result;
}

Status AddToFeeds(const IExecutionProvider* execution_provider, Stream* ort_stream, std::initializer_list<OrtValue> inputs, std::vector<OrtValue>& feeds, IAllocatorUniquePtr<char>& buffer) {
#ifdef ENABLE_NVTX_PROFILE
 profile::NvtxNestedRangeCreator addToFeedsRange("AddToFeeds", profile::Color::Blue);
 addToFeedsRange.Begin();
#endif

 
 const CUDAExecutionProvider* provider = reinterpret_cast<const CUDAExecutionProvider*>(execution_provider);
 size_t total_bytes = 0;
 for (auto& input : inputs) {
  if (input.IsAllocated()) {
   total_bytes += input.Get<Tensor>().SizeInBytes();
  }
 }

 ORT_ENFORCE(total_bytes > 0);

 AllocatorPtr pinned_allocator = provider->GetAllocator(OrtMemTypeCPU);
 cudaStream_t stream = ort_stream ? static_cast<cudaStream_t>(ort_stream->GetHandle()) : nullptr;
 auto pinned_buffer = IAllocator::MakeUniquePtr<void>(pinned_allocator, total_bytes);
 char* pinned_data = static_cast<char*>(pinned_buffer.get());
 
 char* destination = pinned_data;
 for (auto& input : inputs) {
  if (input.IsAllocated()) {
   const Tensor& tensor = input.Get<Tensor>();
   const size_t bytes = tensor.SizeInBytes();
   MLDataType dataType = tensor.DataType();
   if (dataType == DataTypeImpl::GetType<int32_t>()) {
    memcpy(destination, input.Get<Tensor>().Data<int32_t>(), bytes);
   } else if (dataType == DataTypeImpl::GetType<int64_t>()) {
    memcpy(destination, input.Get<Tensor>().Data<int64_t>(), bytes);
   } else if (dataType == DataTypeImpl::GetType<float>()) {
    memcpy(destination, input.Get<Tensor>().Data<float>(), bytes);
   } else if (dataType == DataTypeImpl::GetType<MLFloat16>()) {
    memcpy(destination, input.Get<Tensor>().Data<MLFloat16>(), bytes);
   } else {
    return ORT_MAKE_STATUS(ONNXRUNTIME, NOT_IMPLEMENTED, "AddToFeeds: An implementation for the input type ", dataType, " is not supported yet");
   }
   
   destination += bytes;
  }
 }
 if (!buffer) {
  buffer = provider->GetScratchBuffer<char>(total_bytes, ort_stream, WaitCudaNotificationOnDevice);
 }
 char* gpu_data = buffer.get();
 CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(gpu_data, pinned_data, total_bytes, cudaMemcpyHostToDevice, stream));

 
 onnxruntime::contrib::cuda::AutoDestoryCudaEvent new_event;
 cudaEvent_t& isCopyDone = new_event.Get();
 CUDA_RETURN_IF_ERROR(cudaEventCreate(&isCopyDone));
 CUDA_RETURN_IF_ERROR(cudaEventRecord(isCopyDone, stream));
 CUDA_RETURN_IF_ERROR(cudaEventSynchronize(isCopyDone));
 
 const OrtMemoryInfo& location = provider->GetAllocator(OrtMemTypeDefault)->Info();
 for (auto& input : inputs) {
  if (input.IsAllocated()) {
   const Tensor& tensor = input.Get<Tensor>();
   const TensorShape& shape = tensor.Shape();
   const size_t bytes = tensor.SizeInBytes();
   MLDataType dataType = tensor.DataType();

   OrtValue device_input;
   Tensor::InitOrtValue(dataType, shape, gpu_data, location, device_input);
   gpu_data += bytes;
   feeds.push_back(device_input);
  }
 }

#ifdef ENABLE_NVTX_PROFILE
 addToFeedsRange.End();
#endif

 return Status::OK();
}

template <typename T>
void InitBeamState(transformers::IBeamSearchState<T>* beam_state, gsl::span<int32_t>& sequence_lengths, int batch_size, int num_beams, Stream* ort_stream) {
#ifdef ENABLE_NVTX_PROFILE
 profile::NvtxNestedRangeCreator initStateRange("InitBeamState", profile::Color::Red);
 initStateRange.Begin();
#endif

 
 cudaStream_t cuda_stream = ort_stream ? static_cast<cudaStream_t>(ort_stream->GetHandle()) : nullptr;
 CUDA_CALL_THROW(cudaMemsetAsync(beam_state->next_token_logits.data(), 0, beam_state->next_token_logits.size_bytes(), cuda_stream));
 CUDA_CALL_THROW(cudaMemsetAsync(beam_state->next_token_scores.data(), 0, beam_state->next_token_scores.size_bytes(), cuda_stream));
 CUDA_CALL_THROW(cudaMemsetAsync(beam_state->next_tokens.data(), 0, beam_state->next_tokens.size_bytes(), cuda_stream));
 CUDA_CALL_THROW(cudaMemsetAsync(beam_state->next_indices.data(), 0, beam_state->next_indices.size_bytes(), cuda_stream));
 CUDA_CALL_THROW(cudaMemsetAsync(beam_state->next_scores.data(), 0, beam_state->next_scores.size_bytes(), cuda_stream));
 CUDA_CALL_THROW(cudaMemsetAsync(beam_state->topk_buffer.data(), 0, beam_state->topk_buffer.size_bytes(), cuda_stream));

 
 cuda::LaunchInitKernel(beam_state->beam_scores.data(), batch_size, num_beams, cuda_stream);

 
 
 if (!beam_state->next_positions.empty()) { 
  CUDA_CALL_THROW(cudaMemcpyAsync(beam_state->next_positions.data(), sequence_lengths.data(), sequence_lengths.size_bytes(), cudaMemcpyHostToDevice, cuda_stream));
 }

#ifdef ENABLE_NVTX_PROFILE
 initStateRange.End();
#endif
}

template <typename T>
void InitGreedyState(transformers::IGreedySearchState<T>* greedy_state, gsl::span<int32_t>& sequence_lengths, Stream* ort_stream) {
#ifdef ENABLE_NVTX_PROFILE
 profile::NvtxNestedRangeCreator initStateRange("InitGreedyState", profile::Color::Red);
 initStateRange.Begin();
#endif

 cudaStream_t cuda_stream = ort_stream ? reinterpret_cast<cudaStream_t>(ort_stream->GetHandle()) : nullptr;
 CUDA_CALL_THROW(cudaMemsetAsync(greedy_state->next_token_scores.data(), 0, greedy_state->next_token_scores.size_bytes(), cuda_stream));
 CUDA_CALL_THROW(cudaMemsetAsync(greedy_state->next_positions.data(), 0, greedy_state->next_positions.size_bytes(), cuda_stream));

 CUDA_CALL_THROW(cudaMemcpyAsync(greedy_state->next_positions.data(), sequence_lengths.data(), sequence_lengths.size_bytes(), cudaMemcpyHostToDevice, cuda_stream));

#ifdef ENABLE_NVTX_PROFILE
 initStateRange.End();
#endif
}

template <typename T>
Status ProcessLogits(const OrtValue& logits, transformers::IBeamSearchState<T>* beam_state, transformers::IBeamSearchCpuState* cpu_state, transformers::ISequences* sequences, AllocatorPtr& allocator, onnxruntime::concurrency::ThreadPool* thread_pool, transformers::ILogitsProcessorList* logits_processors, transformers::IBeamScorer* beam_scorer, const transformers::IGenerationParameters* parameters, int step, Stream* ort_stream, const transformers::IConsoleDumper* dumper) {      

#ifdef ENABLE_NVTX_PROFILE
 profile::NvtxNestedRangeCreator processLogitsRange("ProcessLogits", profile::Color::Red);
 processLogitsRange.Begin();
#endif

 ORT_UNUSED_PARAMETER(logits_processors);
 ORT_UNUSED_PARAMETER(thread_pool);

#ifndef DEBUG_GENERATION
 ORT_UNUSED_PARAMETER(dumper);
#endif

 int batch_size = parameters->batch_size;
 int num_beams = parameters->num_beams;
 int vocab_size = parameters->vocab_size;
 bool output_scores = parameters->output_scores;

 int batch_beam_size = batch_size * num_beams;

 typedef typename ToCudaType<T>::MappedType CudaT;
 const CudaT* logits_data = reinterpret_cast<const CudaT*>(logits.Get<Tensor>().Data<T>());

 
 
 const TensorShape& logits_shape = logits.Get<Tensor>().Shape();
 ORT_ENFORCE(logits_shape.NumDimensions() == 3);
 auto input_length = logits_shape[1];
 auto logits_batch_size = logits_shape[0];

 
 
 
 auto padded_vocab_size = static_cast<int>(logits_shape[2]);

 cudaStream_t cuda_stream = ort_stream ? static_cast<cudaStream_t>(ort_stream->GetHandle()) : nullptr;

 
 
 
 gsl::span<T>& next_token_logits = beam_state->next_token_logits;
 
 if (input_length > 1 || logits_batch_size == batch_size) {
  
  
  const CudaT* current_logits = logits_data + (input_length - 1) * padded_vocab_size;
  for (int i = 0; i < batch_beam_size; i++) {
   
   
   
   gsl::span<const T> source(reinterpret_cast<const T*>(current_logits), vocab_size);
   gsl::span<T> target = next_token_logits.subspan(static_cast<size_t>(i) * vocab_size, vocab_size);
   CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(target.data(), source.data(), sizeof(T) * vocab_size, cudaMemcpyDeviceToDevice, cuda_stream));
   if (logits_batch_size == batch_beam_size) {
    current_logits += input_length * padded_vocab_size;
   } else if (logits_batch_size == batch_size && i % num_beams == num_beams - 1) {
    current_logits += input_length * padded_vocab_size;
   }
  }
 }

#ifdef DEBUG_GENERATION
 dumper->Print("logits", logits);
 if (input_length > 1 || logits_batch_size == batch_size) {
  dumper->Print("next_token_logits", next_token_logits.data(), batch_size, num_beams, vocab_size);
 }
#endif

 
 gsl::span<float>& next_token_scores = beam_state->next_token_scores;

 
 float* Y_data = next_token_scores.data();
 bool is_reuse_logits_buffer = (input_length == 1 && logits_batch_size == batch_beam_size);

 const CudaT* X_data = is_reuse_logits_buffer ? logits_data : reinterpret_cast<const CudaT*>(next_token_logits.data());

 ORT_RETURN_IF_ERROR((dispatch_blockwise_softmax_forward<CudaT, float, float, true>(
   cuda_stream, Y_data, X_data, vocab_size, is_reuse_logits_buffer ? padded_vocab_size : vocab_size, vocab_size, batch_size * num_beams)));

#ifdef DEBUG_GENERATION
 dumper->Print("next_token_scores after softmax", next_token_scores.data(), batch_size, num_beams, vocab_size);
#endif

 
 
 BufferUniquePtr sequences_buffer;
 int current_sequence_length = sequences->GetSequenceLength();
 bool run_ngram = parameters->no_repeat_ngram_size > 0 && current_sequence_length >= parameters->no_repeat_ngram_size;
 if (parameters->repetition_penalty != 1.0f || run_ngram) {
  size_t bytes = SafeInt<size_t>(sizeof(int32_t)) * batch_beam_size * parameters->max_length;
  void* data = allocator->Alloc(bytes);
  BufferUniquePtr temp_buffer(data, BufferDeleter(allocator));
  sequences_buffer = std::move(temp_buffer);
  CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(sequences_buffer.get(), sequences->GetSequence(0).data(), bytes, cudaMemcpyHostToDevice, cuda_stream));
 }

 cuda::LaunchLogitsProcessKernel<float>(
   next_token_scores.data(), parameters->vocab_mask.data(), step > 1 ? nullptr : parameters->prefix_vocab_mask.data(), nullptr, parameters->presence_penalty, parameters->temperature, parameters->batch_size, parameters->num_beams, vocab_size, vocab_size, (parameters->min_length > 0 && current_sequence_length < parameters->min_length) ? parameters->eos_token_id : -1, reinterpret_cast<int32_t*>(sequences_buffer.get()), parameters->max_length, current_sequence_length, parameters->repetition_penalty, parameters->no_repeat_ngram_size, cuda_stream);

#ifdef DEBUG_GENERATION
 dumper->Print("next_token_scores after logits process", next_token_scores.data(), batch_size, num_beams, vocab_size);
#endif
 
 
 cuda::LaunchAddProbsKernel(next_token_scores.data(), beam_state->beam_scores.data(), batch_size, num_beams, vocab_size, cuda_stream);

#ifdef DEBUG_GENERATION
 dumper->Print("next_token_scores adding beam_scores", next_token_scores.data(), batch_size, num_beams, vocab_size);
#endif

 if (output_scores) {
  
  CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(beam_state->remaining_scores.data(), next_token_scores.data(), next_token_scores.size_bytes(), cudaMemcpyDeviceToDevice, cuda_stream));
  beam_state->remaining_scores = beam_state->remaining_scores.subspan(next_token_scores.size());
 }

 if (num_beams <= 32) {
  constexpr size_t max_parts_of_vocab = 128;
  size_t candidate_count = SafeInt<size_t>(batch_beam_size) * 2 * num_beams;
  float* topk_tmp_buffer = beam_state->topk_buffer.data();
  float* topk_scores_1st_stage = topk_tmp_buffer;
  int32_t* topk_tokens_1st_stage = reinterpret_cast<int32_t*>(topk_scores_1st_stage + candidate_count * max_parts_of_vocab);
  float* topk_scores_2nd_stage = reinterpret_cast<float*>(topk_tokens_1st_stage + candidate_count * max_parts_of_vocab);
  int32_t* topk_tokens_2nd_stage = reinterpret_cast<int32_t*>(topk_scores_2nd_stage + candidate_count);

  cuda::BeamSearchTopK(next_token_scores.data(), batch_size, num_beams, vocab_size, 2 * num_beams, topk_scores_1st_stage, topk_tokens_1st_stage, topk_scores_2nd_stage, topk_tokens_2nd_stage, beam_state->next_scores.data(), beam_state->next_tokens.data(), beam_state->next_indices.data(), cuda_stream);

  
#ifdef DEBUG_GENERATION
  dumper->Print("next_tokens before scorer", beam_state->next_tokens.data(), batch_size, 2 * num_beams);
  dumper->Print("next_indices before scorer", beam_state->next_indices.data(), batch_size, 2 * num_beams);
  dumper->Print("next_scores before scorer", beam_state->next_scores.data(), batch_size, 2 * num_beams);
#endif

  
  CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(cpu_state->topk_scores.data(), beam_state->next_scores.data(), beam_state->next_scores.size_bytes(), cudaMemcpyDeviceToHost, cuda_stream));
 } else {
  
  
  
  int64_t next_token_scores_dims[] = {batch_size, static_cast<int64_t>(num_beams) * static_cast<int64_t>(vocab_size)};

  TensorShape next_token_scores_shape(&next_token_scores_dims[0], 2);
  auto element_type = DataTypeImpl::GetType<float>();
  OrtValue next_token_scores_value;
  Tensor::InitOrtValue(element_type, next_token_scores_shape, next_token_scores.data(), allocator->Info(), next_token_scores_value);
  const Tensor& input = next_token_scores_value.Get<Tensor>();

  constexpr int axis = 1;
  const unsigned top_k = static_cast<unsigned>(2 * num_beams);
  constexpr bool largest = true;
  constexpr bool sorted = true; 

  std::unique_ptr<Tensor> topk_scores = Tensor::CreateDefault();
  std::unique_ptr<Tensor> topk_indices = Tensor::CreateDefault();
  ORT_RETURN_IF_ERROR(TopK(&input, axis, top_k, largest, sorted, allocator, ort_stream, thread_pool, *topk_scores, *topk_indices));

#ifdef DEBUG_GENERATION
  dumper->Print("topk_scores", *(topk_scores.get()));
  dumper->Print("topk_indices", *(topk_indices.get()));
#endif

  
  
  
  const int64_t* next_token_indices = topk_indices->Data<int64_t>();
  cuda::LaunchNextTokenKernel(next_token_indices, beam_state->next_indices.data(), beam_state->next_tokens.data(), batch_size, top_k, vocab_size, cuda_stream);

  const float* data = topk_scores->Data<float>();
#ifdef DEBUG_GENERATION
  dumper->Print("next_scores before scorer", data, batch_size, top_k);
  dumper->Print("next_tokens before scorer", beam_state->next_tokens.data(), batch_size, top_k);
  dumper->Print("next_indices before scorer", beam_state->next_indices.data(), batch_size, top_k);
#endif

  
  CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(cpu_state->topk_scores.data(), data, topk_scores->SizeInBytes(), cudaMemcpyDeviceToHost, cuda_stream));
 }

 
 CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(cpu_state->topk_tokens.data(), beam_state->next_tokens.data(), beam_state->next_tokens.size_bytes(), cudaMemcpyDeviceToHost, cuda_stream));
 CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(cpu_state->topk_indices.data(), beam_state->next_indices.data(), beam_state->next_indices.size_bytes(), cudaMemcpyDeviceToHost, cuda_stream));
 CUDA_RETURN_IF_ERROR(cudaStreamSynchronize(cuda_stream));

 gsl::span<const float> next_scores(cpu_state->topk_scores.data(), beam_state->next_scores.size());
 gsl::span<const int32_t> next_tokens(cpu_state->topk_tokens.data(), beam_state->next_tokens.size());
 gsl::span<const int32_t> next_indices(cpu_state->topk_indices.data(), beam_state->next_indices.size());

 
 beam_scorer->Process(
   *sequences, next_scores, next_tokens, next_indices);

 
 
 auto chosen_indices = beam_scorer->GetNextIndices();
 auto beam_state_chosen_indices = beam_state->chosen_indices;

 if (!beam_state_chosen_indices.empty()) {
  
  
  
  CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(beam_state_chosen_indices.data(), chosen_indices.data(), chosen_indices.size_bytes(), cudaMemcpyHostToDevice, cuda_stream));

  CUDA_RETURN_IF_ERROR(cudaStreamSynchronize(cuda_stream));
 }

#ifdef ENABLE_NVTX_PROFILE
 processLogitsRange.End();
#endif

 return Status::OK();
}

template <typename T>
Status GreedySearchProcessLogits(
  const OrtValue& logits, transformers::IGreedySearchState<T>* greedy_state, transformers::ISamplingState<T>* sampling_state, transformers::ISequences* sequences, AllocatorPtr& allocator, onnxruntime::concurrency::ThreadPool* thread_pool, transformers::ILogitsProcessorList* logits_processors, const transformers::IGenerationParameters* parameters, bool do_sampling, int step, Stream* stream, const transformers::IConsoleDumper* dumper) {      

#ifdef ENABLE_NVTX_PROFILE
 profile::NvtxNestedRangeCreator processLogitsRange("ProcessLogits", profile::Color::Red);
 processLogitsRange.Begin();
#endif

 ORT_UNUSED_PARAMETER(logits_processors);
 ORT_UNUSED_PARAMETER(thread_pool);
#ifndef DEBUG_GENERATION
 ORT_UNUSED_PARAMETER(dumper);
#endif

 int batch_size = parameters->batch_size;
 int vocab_size = parameters->vocab_size;
 bool output_scores = parameters->output_scores;

 int batch_beam_size = batch_size;

 typedef typename ToCudaType<T>::MappedType CudaT;
 const CudaT* logits_data = reinterpret_cast<const CudaT*>(logits.Get<Tensor>().Data<T>());

 
 
 const TensorShape& logits_shape = logits.Get<Tensor>().Shape();
 ORT_ENFORCE(logits_shape.NumDimensions() == 3);
 auto input_length = logits_shape[1];

 
 
 
 auto padded_vocab_size = static_cast<int>(logits_shape[2]);

 cudaStream_t cuda_stream = stream ? reinterpret_cast<cudaStream_t>(stream->GetHandle()) : nullptr;

 
 
 
 gsl::span<T>& next_token_scores = greedy_state->next_token_scores;

 
 
 
 auto is_reuse_logits_buffer = !do_sampling && (input_length == 1);

 
 
 if (!is_reuse_logits_buffer) {
  

  
  
  const CudaT* current_logits = logits_data + (input_length - 1) * padded_vocab_size;
  for (ptrdiff_t i = 0; i < batch_beam_size; i++) {
   
   
   
   gsl::span<const T> source(reinterpret_cast<const T*>(current_logits), vocab_size);
   gsl::span<T> target = next_token_scores.subspan(i * vocab_size, vocab_size);
   CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(target.data(), source.data(), sizeof(T) * vocab_size, cudaMemcpyDeviceToDevice, cuda_stream));
   current_logits += input_length * padded_vocab_size;
  }
 }

#ifdef DEBUG_GENERATION
 dumper->Print("logits", logits);
 if (is_reuse_logits_buffer) {
  
  ORT_THROW("Dumping contents of logits buffer is not implemented yet");
 } else {
  dumper->Print("next_token_scores", next_token_scores.data(), batch_size, vocab_size);
 }
#endif

 
 
 BufferUniquePtr sequences_buffer;
 int current_sequence_length = sequences->GetSequenceLength();
 if (parameters->repetition_penalty != 1.0f) {
  size_t bytes = SafeInt<size_t>(sizeof(int32_t)) * batch_beam_size * parameters->max_length;
  void* data = allocator->Alloc(bytes);
  BufferUniquePtr temp_buffer(data, BufferDeleter(allocator));
  sequences_buffer = std::move(temp_buffer);
  CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(sequences_buffer.get(), sequences->GetSequence(0).data(), bytes, cudaMemcpyHostToDevice, cuda_stream));
 }

 
 gsl::span<int>& presence_mask = sampling_state->d_presence_mask;
 if (step == 1 && parameters->presence_mask.data() != nullptr) {
  CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(presence_mask.data(), parameters->presence_mask.data(), sizeof(int) * batch_size * vocab_size, cudaMemcpyDeviceToDevice, cuda_stream));
 }

 
 
 
 
 cuda::LaunchLogitsProcessKernel<CudaT>(
   is_reuse_logits_buffer ? const_cast<CudaT*>(logits_data)
               : reinterpret_cast<CudaT*>(next_token_scores.data()), parameters->vocab_mask.data(), step > 1 ? nullptr : parameters->prefix_vocab_mask.data(), parameters->presence_mask.data() ? presence_mask.data() : nullptr, parameters->presence_penalty, parameters->temperature, parameters->batch_size, parameters->num_beams, vocab_size, is_reuse_logits_buffer ? padded_vocab_size : vocab_size, (parameters->min_length > 0 && current_sequence_length < parameters->sequence_length + parameters->min_length)
     ? parameters->eos_token_id
     : -1, reinterpret_cast<int32_t*>(sequences_buffer.get()), parameters->max_length, current_sequence_length, parameters->repetition_penalty, parameters->no_repeat_ngram_size, cuda_stream);

#ifdef DEBUG_GENERATION
 if (is_reuse_logits_buffer) {
  
  ORT_THROW("Dumping contents of logits buffer is not implemented yet");
 } else {
  dumper->Print("next_token_scores after logits process", next_token_scores.data(), batch_size, vocab_size);
 }
#endif

 
 ORT_UNUSED_PARAMETER(output_scores);

 if (do_sampling) {
  ORT_RETURN_IF_ERROR(SamplingCudaHelper::Sample(allocator, cuda_stream, next_token_scores, sampling_state, greedy_state, parameters, step, dumper));

  return Status::OK();
 }

 const CudaT* top_one_input = is_reuse_logits_buffer ? logits_data
                           : reinterpret_cast<const CudaT*>(next_token_scores.data());
 cuda::GreedySearchTopOne(
   top_one_input, batch_size, is_reuse_logits_buffer ? padded_vocab_size : vocab_size, reinterpret_cast<CudaT*>(greedy_state->temp_topk_scores_buffer.data()), greedy_state->temp_topk_tokens_buffer.data(), reinterpret_cast<CudaT*>(greedy_state->topk_scores_buffer.data()), greedy_state->topk_tokens_buffer.data(), cuda_stream);

#ifdef DEBUG_GENERATION
 dumper->Print("topk_scores", greedy_state->topk_scores_buffer.data(), batch_size, 1);
 dumper->Print("topk_indices", greedy_state->topk_tokens_buffer.data(), batch_size, 1);
#endif

 CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(greedy_state->next_tokens.data(), greedy_state->topk_tokens_buffer.data(), greedy_state->next_tokens.size_bytes(), cudaMemcpyDeviceToHost, cuda_stream));
 CUDA_RETURN_IF_ERROR(cudaStreamSynchronize(cuda_stream));

#ifdef DEBUG_GENERATION
 dumper->Print("greedy_state->next_tokens", greedy_state->next_tokens.data(), batch_size, 1);
#endif

#ifdef ENABLE_NVTX_PROFILE
 processLogitsRange.End();
#endif

 return Status::OK();
}

template <typename T>
Status DeviceCopy(gsl::span<T> target, gsl::span<const T> source, Stream* ort_stream, int copyDirection) {
 assert(copyDirection >= 0 && copyDirection <= 3);
 cudaStream_t cuda_stream = ort_stream ? static_cast<cudaStream_t>(ort_stream->GetHandle()) : nullptr;
 if (cuda_stream == nullptr) {
  CUDA_RETURN_IF_ERROR(cudaMemcpy(target.data(), source.data(), source.size_bytes(), static_cast<cudaMemcpyKind>(copyDirection)));
 } else {
  CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(target.data(), source.data(), source.size_bytes(), static_cast<cudaMemcpyKind>(copyDirection), cuda_stream));
  CUDA_RETURN_IF_ERROR(cudaStreamSynchronize(cuda_stream));
 }
 return Status::OK();
}

template <typename T>
Status PickGptPastState(const std::vector<OrtValue>& last_outputs, std::vector<OrtValue>& next_inputs, gsl::span<const int32_t>& beam_indices, AllocatorPtr allocator, ptrdiff_t gpt_subgraph_first_past_input_idx, ptrdiff_t gpt_subgraph_first_present_output_idx, Stream* ort_stream) {
 ptrdiff_t num_present_tensors = static_cast<ptrdiff_t>(last_outputs.size()) - gpt_subgraph_first_present_output_idx;
 for (int i = 0; i < num_present_tensors; ++i) {
  const OrtValue& present = last_outputs[gpt_subgraph_first_present_output_idx + i];

  
  const TensorShape& past_shape = present.Get<Tensor>().Shape();
  auto block_size_per_beam = past_shape[2] * past_shape[3] * past_shape[4];
  auto past_key_size = past_shape[1] * past_shape[2] * past_shape[3] * past_shape[4];

  
  
  OrtValue past;
  auto past_type = DataTypeImpl::GetType<T>();
  Tensor::InitOrtValue(past_type, past_shape, allocator, past);
  cudaStream_t cuda_stream = ort_stream ? static_cast<cudaStream_t>(ort_stream->GetHandle()) : nullptr;

  gsl::span<T> past_span = gsl::make_span<T>(past.GetMutable<Tensor>()->MutableData<T>(), past_shape.Size());
  gsl::span<const T> present_span = gsl::make_span<const T>(present.Get<Tensor>().Data<T>(), past_shape.Size());
  for (size_t j = 0; j < beam_indices.size(); j++) {
   int32_t beam_index = beam_indices[j];
   gsl::span<const T> present_key = present_span.subspan(beam_index * block_size_per_beam, block_size_per_beam);
   gsl::span<const T> present_value = present_span.subspan(past_key_size + beam_index * block_size_per_beam, block_size_per_beam);

   gsl::span<T> past_key = past_span.subspan(j * block_size_per_beam, block_size_per_beam);
   gsl::span<T> past_value = past_span.subspan(past_key_size + j * block_size_per_beam, block_size_per_beam);
   CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(past_key.data(), present_key.data(), present_key.size_bytes(), cudaMemcpyDeviceToDevice, cuda_stream));
   CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(past_value.data(), present_value.data(), present_value.size_bytes(), cudaMemcpyDeviceToDevice, cuda_stream));
  }

  next_inputs[gpt_subgraph_first_past_input_idx + i] = past;
 }

 return Status::OK();
}


template <typename T>
Status PickT5PastState(const std::vector<OrtValue>& last_outputs, std::vector<OrtValue>& next_inputs, int num_present_tensors, gsl::span<const int32_t>& beam_indices, AllocatorPtr allocator, ptrdiff_t t5_decoder_first_past_input_idx, ptrdiff_t t5_decoder_first_present_output_idx, Stream* ort_stream) {
 cudaStream_t cuda_stream = ort_stream ? static_cast<cudaStream_t>(ort_stream->GetHandle()) : nullptr;
 for (int i = 0; i < num_present_tensors; ++i) {
  const OrtValue& present = last_outputs[t5_decoder_first_present_output_idx + i];

  
  const TensorShape& past_shape = present.Get<Tensor>().Shape();
  auto block_size_per_beam = past_shape[1] * past_shape[2] * past_shape[3];

  
  
  OrtValue past;
  Tensor::InitOrtValue(DataTypeImpl::GetType<T>(), past_shape, allocator, past);

  gsl::span<T> past_span = gsl::make_span<T>(past.GetMutable<Tensor>()->MutableData<T>(), past_shape.Size());
  gsl::span<const T> present_span = gsl::make_span<const T>(present.Get<Tensor>().Data<T>(), past_shape.Size());
  for (size_t j = 0; j < beam_indices.size(); j++) {
   int32_t beam_index = beam_indices[j];
   gsl::span<const T> present_beam = present_span.subspan(beam_index * block_size_per_beam, block_size_per_beam);
   gsl::span<T> past_beam = past_span.subspan(j * block_size_per_beam, block_size_per_beam);
   CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(past_beam.data(), present_beam.data(), present_beam.size_bytes(), cudaMemcpyDeviceToDevice, cuda_stream));
  }

  next_inputs[t5_decoder_first_past_input_idx + i] = past;
 }

 return Status::OK();
}

template <typename T>
Status UpdateGptFeeds(
  AllocatorPtr allocator, Stream* ort_stream, const std::vector<OrtValue>& last_outputs, std::vector<OrtValue>& next_inputs, int current_length, OrtValue& position_ids, bool increase_position, gsl::span<const int32_t> beam_next_tokens, gsl::span<const int32_t> beam_indices_cpu, gsl::span<const int32_t> beam_indices_gpu, int num_beams, int gpt_subgraph_first_past_input_idx, int gpt_subgraph_first_present_output_idx, bool past_present_share_buffer, int past_sequence_len, int input_sequence_len, bool need_cache_indir) {
#ifdef ENABLE_NVTX_PROFILE
 profile::NvtxNestedRangeCreator updateFeedsRange("UpdateGptFeeds", profile::Color::Yellow);
 updateFeedsRange.Begin();
#endif

 
 int batch_beam_size = static_cast<int>(beam_next_tokens.size());
 int64_t dims[] = {batch_beam_size, 1};
 TensorShape input_ids_shape(&dims[0], 2);
 auto element_type = DataTypeImpl::GetType<int32_t>();
 OrtValue input_ids;
 Tensor::InitOrtValue(element_type, input_ids_shape, allocator, input_ids);
 int32_t* input_ids_data = input_ids.GetMutable<Tensor>()->MutableData<int32_t>();
 cudaStream_t cuda_stream = ort_stream ? static_cast<cudaStream_t>(ort_stream->GetHandle()) : nullptr;

 
 CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(input_ids_data, beam_next_tokens.data(), beam_next_tokens.size_bytes(), cudaMemcpyHostToDevice, cuda_stream));
 next_inputs[0] = input_ids;

 
 int32_t* position_data = increase_position ? position_ids.GetMutable<Tensor>()->MutableData<int32_t>() : nullptr;
 next_inputs[1] = position_ids;

 
 const OrtValue& old_mask = next_inputs[2];
 const int32_t* old_mask_data = old_mask.Get<Tensor>().Data<int32_t>();
 int64_t mask_dims[] = {batch_beam_size, current_length};
 TensorShape mask_shape(&mask_dims[0], 2);
 OrtValue attention_mask;
 auto mask_type = DataTypeImpl::GetType<int32_t>();
 Tensor::InitOrtValue(mask_type, mask_shape, allocator, attention_mask);
 int32_t* mask_data = attention_mask.GetMutable<Tensor>()->MutableData<int32_t>();

 
 cuda::LaunchUpdateGptKernel(old_mask_data, mask_data, position_data, batch_beam_size, current_length, cuda_stream);

 next_inputs[2] = attention_mask;

 if (past_present_share_buffer) {
  
  const ptrdiff_t past_sequence_length_idx = (static_cast<ptrdiff_t>(last_outputs.size()) - gpt_subgraph_first_present_output_idx) + gpt_subgraph_first_past_input_idx;
  *(next_inputs[past_sequence_length_idx].GetMutable<Tensor>()->MutableData<int32_t>()) = past_sequence_len;

  

  
  
  if (need_cache_indir) {
   ORT_ENFORCE(!beam_indices_gpu.empty(), "Beam indices must be present on CUDA while using DecoderMaskedSelfAttention with BeamSearch");

   
   const OrtValue& old_cache_indirection = next_inputs[past_sequence_length_idx + 2];

   
   OrtValue cache_indirection;

   Tensor::InitOrtValue(DataTypeImpl::GetType<int32_t>(), old_cache_indirection.Get<Tensor>().Shape(), allocator, cache_indirection);

   
   int max_sequence_length = static_cast<int>(last_outputs[gpt_subgraph_first_present_output_idx].Get<Tensor>().Shape()[3]);

   
   cuda::UpdateDecoderMaskedMultiHeadAttentionCacheIndirection(cache_indirection.GetMutable<Tensor>()->MutableData<int32_t>(), old_cache_indirection.Get<Tensor>().Data<int32_t>(), reinterpret_cast<const int32_t*>(beam_indices_gpu.data()), batch_beam_size / num_beams, num_beams, input_sequence_len, max_sequence_length, current_length, cuda_stream);
   
   next_inputs[past_sequence_length_idx + 2] = cache_indirection;
  }
 } else {
  if (num_beams == 1) {
   const int k = gpt_subgraph_first_past_input_idx - gpt_subgraph_first_present_output_idx;
   
   for (size_t i = gpt_subgraph_first_present_output_idx; i < last_outputs.size(); ++i) {
    next_inputs[i + k] = last_outputs[i];
   }
  } else {
   ORT_RETURN_IF_ERROR(PickGptPastState<T>(last_outputs, next_inputs, beam_indices_cpu, allocator, gpt_subgraph_first_past_input_idx, gpt_subgraph_first_present_output_idx, ort_stream));
  }
 }

 
 CUDA_RETURN_IF_ERROR(cudaStreamSynchronize(cuda_stream));

#ifdef ENABLE_NVTX_PROFILE
 updateFeedsRange.End();
#endif

 return Status::OK();
}


template <typename T>
Status UpdateDecoderFeeds(
  AllocatorPtr allocator, Stream* ort_stream, const std::vector<OrtValue>& last_outputs, std::vector<OrtValue>& next_inputs, int num_present_tensors, gsl::span<const int32_t> beam_next_tokens, gsl::span<const int32_t> beam_indices, gsl::span<const int32_t> beam_indices_gpu, int num_beams, int t5_decoder_first_past_input_idx, int t5_decoder_first_present_output_idx, bool use_sequence_as_input_ids, int current_length, int input_sequence_len, bool past_present_share_buffer, bool need_cache_indir, transformers::Sequences& sequences, const transformers::IConsoleDumper* dumper) {
 
 
 
 
 
 

 
 int batch_beam_size = gsl::narrow<int>(beam_next_tokens.size());
 int sequence_length = !use_sequence_as_input_ids ? 1 : current_length;
 TensorShape input_ids_shape{batch_beam_size, sequence_length};
 auto element_type = DataTypeImpl::GetType<int32_t>();
 OrtValue input_ids;
 Tensor::InitOrtValue(element_type, input_ids_shape, allocator, input_ids);
 int32_t* input_ids_data = input_ids.GetMutable<Tensor>()->MutableData<int32_t>();
 cudaStream_t cuda_stream = ort_stream ? static_cast<cudaStream_t>(ort_stream->GetHandle()) : nullptr;

 if (!use_sequence_as_input_ids) {
  CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(input_ids_data, beam_next_tokens.data(), beam_next_tokens.size_bytes(), cudaMemcpyHostToDevice, cuda_stream));
 } else {
  for (int i = 0; i < batch_beam_size; i++) {
   gsl::span<const int32_t> sequence = sequences.GetSequence(i);
   const int32_t* sequence_data = sequence.data();
   CUDA_RETURN_IF_ERROR(
     cudaMemcpyAsync(input_ids_data + static_cast<ptrdiff_t>(i) * current_length, sequence_data, current_length * sizeof(int32_t), cudaMemcpyHostToDevice, cuda_stream));
  }
 }
 next_inputs[0] = input_ids;

#ifdef DEBUG_GENERATION
 dumper->Print("input_ids", input_ids);
#else
 ORT_UNUSED_PARAMETER(dumper);
#endif

 
 ORT_ENFORCE(last_outputs.size() >= static_cast<size_t>(num_present_tensors) + 1);

 if (past_present_share_buffer) {
  
  const ptrdiff_t past_sequence_length_idx = 2 * (static_cast<ptrdiff_t>(last_outputs.size()) - t5_decoder_first_present_output_idx) + t5_decoder_first_past_input_idx;
  *(next_inputs[past_sequence_length_idx].GetMutable<Tensor>()->MutableData<int32_t>()) = current_length - 1;

  

  
  
  if (need_cache_indir) {
   ORT_ENFORCE(!beam_indices_gpu.empty(), "Beam indices must be present on CUDA while using DecoderMaskedMultiHeadAttention with BeamSearch");

   
   const OrtValue& old_cache_indirection = next_inputs[past_sequence_length_idx + 2];

   
   OrtValue cache_indirection;

   Tensor::InitOrtValue(DataTypeImpl::GetType<int32_t>(), old_cache_indirection.Get<Tensor>().Shape(), allocator, cache_indirection);

   
   int max_sequence_length = static_cast<int>(last_outputs[t5_decoder_first_present_output_idx].Get<Tensor>().Shape()[2]);

   
   cuda::UpdateDecoderMaskedMultiHeadAttentionCacheIndirection(cache_indirection.GetMutable<Tensor>()->MutableData<int32_t>(), old_cache_indirection.Get<Tensor>().Data<int32_t>(), reinterpret_cast<const int32_t*>(beam_indices_gpu.data()), batch_beam_size / num_beams, num_beams, input_sequence_len, max_sequence_length, current_length, cuda_stream);

   
   next_inputs[past_sequence_length_idx + 2] = cache_indirection;
  }
 } else {
  
  if (num_beams == 1) {
   
   for (ptrdiff_t i = 0; i < num_present_tensors; ++i) {
    next_inputs[t5_decoder_first_past_input_idx + i] =
      last_outputs[t5_decoder_first_present_output_idx + i];
   }
   return Status::OK();
  }

  return PickT5PastState<T>(last_outputs, next_inputs, num_present_tensors, beam_indices, allocator, t5_decoder_first_past_input_idx, t5_decoder_first_present_output_idx, ort_stream);
 }

 
 CUDA_RETURN_IF_ERROR(cudaStreamSynchronize(cuda_stream));

 return Status::OK();
}

namespace {
template <typename T>
struct ToCudaTypeWrapper : public ToCudaType<T> {};

template <>
struct ToCudaTypeWrapper<int32_t> {
 using MappedType = int32_t;
};
} 

template <typename T>
Status ExpandBuffer(Stream* ort_stream, const OrtValue& input, int num_beams, AllocatorPtr allocator, OrtValue& expanded, bool only_copy_shape, int max_sequence_length) {
 
 
 const TensorShape& input_shape = input.Get<Tensor>().Shape();
 const int64_t& batch_size = input_shape[0];
 int64_t sequence_length = 0;

 int64_t dims[4] = {0};
 input_shape.CopyDims(dims, input_shape.NumDimensions());
 dims[0] = batch_size * num_beams;
 bool is_kv_cache = input_shape.NumDimensions() == 4;
 if (max_sequence_length > 0 && is_kv_cache) {
  sequence_length = input_shape[2];
  dims[2] = max_sequence_length;
 }
 TensorShape expanded_shape(&dims[0], input_shape.NumDimensions());

 MLDataType element_type = input.Get<Tensor>().DataType();
 ORT_ENFORCE(element_type == DataTypeImpl::GetType<T>());
 Tensor::InitOrtValue(element_type, expanded_shape, allocator, expanded);

 if (only_copy_shape) {
  return Status::OK();
 }

 cudaStream_t cuda_stream = ort_stream ? static_cast<cudaStream_t>(ort_stream->GetHandle()) : nullptr;

 const T* input_data = input.Get<Tensor>().Data<T>();
 T* expanded_data = expanded.GetMutable<Tensor>()->MutableData<T>();

 using CudaT = typename ToCudaTypeWrapper<T>::MappedType;

 if (max_sequence_length == 0) {
  const int64_t& chunk_size = static_cast<int64_t>(input_shape.Size() / batch_size);

  cuda::BufferExpansionKernelLauncher<CudaT>(reinterpret_cast<const CudaT*>(input_data), reinterpret_cast<CudaT*>(expanded_data), static_cast<int>(batch_size), num_beams, static_cast<int>(chunk_size), cuda_stream);
  return Status::OK();
 }

 ORT_ENFORCE(is_kv_cache);

 
 const int64_t& num_heads = input_shape[1];
 const int64_t& head_size = input_shape[3];

 cuda::KeyCacheExpansionKernelLauncher<CudaT>(reinterpret_cast<const CudaT*>(input_data), reinterpret_cast<CudaT*>(expanded_data), static_cast<int>(batch_size), num_beams, static_cast<int>(num_heads), static_cast<int>(sequence_length), max_sequence_length, static_cast<int>(head_size), cuda_stream);

 return Status::OK();
}


template void InitBeamState<float>(
  transformers::IBeamSearchState<float>* beam_state, gsl::span<int32_t>& sequence_lengths, int batch_size, int num_beams, Stream* ort_stream);

template void InitGreedyState<float>(
  transformers::IGreedySearchState<float>* greedy_state, gsl::span<int32_t>& sequence_lengths, Stream* ort_stream);

template Status ProcessLogits<float>(
  const OrtValue& logits, transformers::IBeamSearchState<float>* beam_state, transformers::IBeamSearchCpuState* cpu_state, transformers::ISequences* sequences, AllocatorPtr& allocator, onnxruntime::concurrency::ThreadPool* thread_pool, transformers::ILogitsProcessorList* logits_processors, transformers::IBeamScorer* beam_scorer, const transformers::IGenerationParameters* parameters, int step, Stream* ort_stream, const transformers::IConsoleDumper* dumper);

template Status GreedySearchProcessLogits<float>(
  const OrtValue& logits, transformers::IGreedySearchState<float>* greedy_state, transformers::ISamplingState<float>* sampling_state, transformers::ISequences* sequences, AllocatorPtr& allocator, onnxruntime::concurrency::ThreadPool* thread_pool, transformers::ILogitsProcessorList* logits_processors, const transformers::IGenerationParameters* parameters, bool do_sampling, int step, Stream* ort_stream, const transformers::IConsoleDumper* dumper);

template Status DeviceCopy<float>(
  gsl::span<float> target, gsl::span<const float> source, Stream* ort_stream, int copyDirection);

template Status DeviceCopy<int32_t>(
  gsl::span<int32_t> target, gsl::span<const int32_t> source, Stream* ort_stream, int copyDirection);

template Status UpdateGptFeeds<float>(
  AllocatorPtr allocator, Stream* ort_stream, const std::vector<OrtValue>& last_outputs, std::vector<OrtValue>& next_inputs, int current_length, OrtValue& position_ids, bool increase_position, gsl::span<const int32_t> beam_next_tokens, gsl::span<const int32_t> beam_indices_cpu, gsl::span<const int32_t> beam_indices_gpu, int num_beams, int gpt_subgraph_first_past_input_idx, int gpt_subgraph_first_present_output_idx, bool past_present_share_buffer, int past_sequence_len, int input_sequence_len, bool need_cache_indir);


template void InitBeamState<MLFloat16>(
  transformers::IBeamSearchState<MLFloat16>* beam_state, gsl::span<int32_t>& sequence_lengths, int batch_size, int num_beams, Stream* ort_stream);

template void InitGreedyState<MLFloat16>(
  transformers::IGreedySearchState<MLFloat16>* greedy_state, gsl::span<int32_t>& sequence_lengths, Stream* ort_stream);

template Status ProcessLogits<MLFloat16>(
  const OrtValue& logits, transformers::IBeamSearchState<MLFloat16>* beam_state, transformers::IBeamSearchCpuState* cpu_state, transformers::ISequences* sequences, AllocatorPtr& allocator, onnxruntime::concurrency::ThreadPool* thread_pool, transformers::ILogitsProcessorList* logits_processors, transformers::IBeamScorer* beam_scorer, const transformers::IGenerationParameters* parameters, int step, Stream* ort_stream, const transformers::IConsoleDumper* dumper);

template Status GreedySearchProcessLogits<MLFloat16>(
  const OrtValue& logits, transformers::IGreedySearchState<MLFloat16>* greedy_state, transformers::ISamplingState<MLFloat16>* sampling_state, transformers::ISequences* sequences, AllocatorPtr& allocator, onnxruntime::concurrency::ThreadPool* thread_pool, transformers::ILogitsProcessorList* logits_processors, const transformers::IGenerationParameters* parameters, bool do_sampling, int step, Stream* ort_stream, const transformers::IConsoleDumper* dumper);

template Status UpdateGptFeeds<MLFloat16>(
  AllocatorPtr allocator, Stream* ort_stream, const std::vector<OrtValue>& last_outputs, std::vector<OrtValue>& next_inputs, int current_length, OrtValue& position_ids, bool increase_position, gsl::span<const int32_t> beam_next_tokens, gsl::span<const int32_t> beam_indices_cpu, gsl::span<const int32_t> beam_indices_gpu, int num_beams, int gpt_subgraph_first_past_input_idx, int gpt_subgraph_first_present_output_idx, bool past_present_share_buffer, int past_sequence_len, int input_sequence_len, bool need_cache_indir);

template Status UpdateDecoderFeeds<float>(
  AllocatorPtr allocator, Stream* ort_stream, const std::vector<OrtValue>& last_outputs, std::vector<OrtValue>& next_inputs, int num_present_tensors, gsl::span<const int32_t> beam_next_tokens, gsl::span<const int32_t> beam_indices, gsl::span<const int32_t> beam_indices_gpu, int num_beams, int t5_decoder_first_past_input_idx, int t5_decoder_first_present_output_idx, bool use_sequence_as_input_ids, int current_length, int input_sequence_len, bool past_present_share_buffer, bool need_cache_indir, transformers::Sequences& sequences, const transformers::IConsoleDumper* dumper);

template Status UpdateDecoderFeeds<MLFloat16>(
  AllocatorPtr allocator, Stream* ort_stream, const std::vector<OrtValue>& last_outputs, std::vector<OrtValue>& next_inputs, int num_present_tensors, gsl::span<const int32_t> beam_next_tokens, gsl::span<const int32_t> beam_indices, gsl::span<const int32_t> beam_indices_gpu, int num_beams, int t5_decoder_first_past_input_idx, int t5_decoder_first_present_output_idx, bool use_sequence_as_input_ids, int current_length, int input_sequence_len, bool past_present_share_buffer, bool need_cache_indir, transformers::Sequences& sequences, const transformers::IConsoleDumper* dumper);

template Status ExpandBuffer<int32_t>(
  Stream* ort_stream, const OrtValue& input, int num_beams, AllocatorPtr allocator, OrtValue& expanded, bool only_copy_shape, int max_sequence_length);

template Status ExpandBuffer<float>(
  Stream* ort_stream, const OrtValue& input, int num_beams, AllocatorPtr allocator, OrtValue& expanded, bool only_copy_shape, int max_sequence_length);

template Status ExpandBuffer<MLFloat16>(
  Stream* ort_stream, const OrtValue& input, int num_beams, AllocatorPtr allocator, OrtValue& expanded, bool only_copy_shape, int max_sequence_length);
} 
} 
} 
