


#if defined(ORT_USE_NCCL) || defined(USE_MPI)

#include "orttraining/training_ops/rocm/communication/send.h"
#include "orttraining/training_ops/communication_common.h"
#include "orttraining/training_ops/rocm/communication/nccl_service.h"
#include "core/providers/rocm/nvtx_profile.h"
#include "core/providers/rocm/nvtx_profile_context.h"
#include "core/providers/rocm/rocm_check_memory.h"
#include "core/providers/rocm/rocm_common.h"

#include "orttraining/core/framework/communication/mpi/mpi_include.h"
#include "orttraining/core/framework/communication/mpi/mpi_context.h"

namespace onnxruntime {
namespace rocm {

ONNX_OPERATOR_KERNEL_EX(
  Send, kMSDomain, 1, kRocmExecutionProvider, (*KernelDefBuilder::Create())
    .InputMemoryType(OrtMemTypeCPUInput, 0)  
    .InputMemoryType(OrtMemTypeCPUInput, 1)  
    .OutputMemoryType(OrtMemTypeCPUOutput, 0) 
    .TypeConstraint("TBool", DataTypeImpl::GetTensorType<bool>())
    .TypeConstraint("TInt64", DataTypeImpl::GetTensorType<int64_t>())
    .TypeConstraint("V", DataTypeImpl::AllFixedSizeTensorTypes()), Send);

void Send::SendData(
  OpKernelContext* ctx, const int dst, const int num_tensors, size_t aggregated_aligned_tensor_bytes, std::vector<size_t> tensor_offsets_in_bytes, std::vector<size_t> tensor_sizes_in_bytes) const {
#ifdef ENABLE_NVTX_PROFILE
 auto& profile_context = profile::Context::GetInstance();
 const auto tag = profile_context.GetThreadTagOrDefault(std::this_thread::get_id());

 profile::NvtxRangeCreator memcpyRange(
   "Batch-" + tag +
     " SendMemcpy-" + std::to_string(dst), profile::Color::Red);
 
 
 
 memcpyRange.Begin();
#endif

#if defined(ORT_USE_NCCL) && defined(USE_NCCL_P2P)
 IAllocatorUniquePtr<char> buffer = GetScratchBuffer<char>(aggregated_aligned_tensor_bytes, ctx->GetComputeStream());
#else
 IAllocatorUniquePtr<char> buffer = AllocateBufferOnCPUPinned<char>(
   aggregated_aligned_tensor_bytes);
#endif

 for (int i = 0; i < num_tensors; ++i) {
  const Tensor* tensor = ctx->Input<Tensor>(i + 2);
#ifndef NDEBUG
  CheckIfMemoryOnCurrentGpuDevice(tensor->DataRaw());
#endif

#if defined(ORT_USE_NCCL) && defined(USE_NCCL_P2P)
  HIP_CALL_THROW(hipMemcpyAsync(buffer.get() + tensor_offsets_in_bytes[i], tensor->DataRaw(), tensor_sizes_in_bytes[i], hipMemcpyDeviceToDevice, Stream(ctx)));
#else
  HIP_CALL_THROW(hipMemcpyAsync(buffer.get() + tensor_offsets_in_bytes[i], tensor->DataRaw(), tensor_sizes_in_bytes[i], hipMemcpyDeviceToHost, Stream(ctx)));
#endif
 }

#ifdef ENABLE_NVTX_PROFILE
 memcpyRange.End();
#endif

#ifdef ENABLE_NVTX_PROFILE
 profile::NvtxRangeCreator sendRange(
   "Batch-" + tag +
     " Send-" + std::to_string(dst), profile::Color::Red);
 
 
 
 sendRange.Begin();
#endif

 CommInfo_t info_data{buffer.get(), static_cast<int>(aggregated_aligned_tensor_bytes), dst, static_cast<int>(tag_)};

#if defined(ORT_USE_NCCL) && defined(USE_NCCL_P2P)
#ifndef NDEBUG
 CheckIfMemoryOnCurrentGpuDevice(info_data.buffer);
#endif

 auto& nccl_service = rocm::NcclService::GetInstance();
 nccl_service.SubmitSendAndWait(info_data.buffer, info_data.size, info_data.rank);
#elif defined(USE_MPI)
 MPI_CHECK(MPI_Send(
   info_data.buffer, info_data.size, MPI_CHAR, info_data.rank, info_data.tag, MPI_COMM_WORLD));
#else
 ORT_THROW("Failed to send to rank: ", info_data.rank);
#endif

#ifdef ENABLE_NVTX_PROFILE
 
 sendRange.End();
#endif
}

Status Send::ComputeInternal(OpKernelContext* ctx) const {
 
 const Tensor* input_signal_tensor = ctx->Input<Tensor>(0);
 const bool* input_signal = input_signal_tensor->template Data<bool>();
 ORT_ENFORCE(*input_signal, "Input control signal of Send must be true before executing the node.");

 
 const Tensor* remote_rank_tensor = ctx->Input<Tensor>(1);
 const int64_t* remote_rank = remote_rank_tensor->template Data<int64_t>();
 const int dst = static_cast<int>(*remote_rank);

 
 int world_rank;
#ifdef USE_MPI
 MPI_CHECK(MPI_Comm_rank(MPI_COMM_WORLD, &world_rank));
#endif
 ORT_ENFORCE(world_rank != dst, "Sending data to rank ", dst, " on the rank ", world_rank, ".");

#ifdef ENABLE_NVTX_PROFILE
 auto& profile_context = profile::Context::GetInstance();
 const auto tag = profile_context.GetThreadTagOrDefault(std::this_thread::get_id());

 profile::NvtxRangeCreator preRange(
   "Batch-" + tag +
     " PreSend-" + std::to_string(dst), profile::Color::Red);
 
 
 preRange.Begin();
#endif

 const int num_tensors = static_cast<int>(element_types_.size());
 std::vector<size_t> tensor_sizes_in_bytes;
 std::vector<TensorShape> tensor_shapes;
 GetTensorShapesAndSizes(
   true, 2, num_tensors, ctx, tensor_sizes_in_bytes, tensor_shapes);

 
 
 size_t aggregated_aligned_tensor_bytes = 0;
 std::vector<size_t> prefix_tensor_shape_sizes;
 std::vector<int64_t> aggregated_tensor_shapes;
 
 std::vector<size_t> tensor_offsets_in_bytes;

 
 
 ComputeShapeRelatedInfo(
   tensor_sizes_in_bytes, tensor_shapes, aggregated_aligned_tensor_bytes, prefix_tensor_shape_sizes, aggregated_tensor_shapes, tensor_offsets_in_bytes);

 bool all_shapes_inferred = true;
 for (int i = 0; i < num_tensors; ++i) {
  TensorShape inferred_shape;
  auto shape_inferred = ctx->TryGetInferredInputShape(i + 2, inferred_shape);
  if (!shape_inferred) {
   all_shapes_inferred = false;
   break;
  }
 }

 
 if (!all_shapes_inferred) {
#ifdef USE_MPI
  SendShapeInfo(dst, tag_, num_tensors, aggregated_aligned_tensor_bytes, prefix_tensor_shape_sizes, aggregated_tensor_shapes);
#else
  ORT_THROW("ORT must be built with MPI to send shape info.");
#endif
 }
#ifdef ENABLE_NVTX_PROFILE
 
 preRange.End();
#endif

 
 SendData(ctx, dst, num_tensors, aggregated_aligned_tensor_bytes, tensor_offsets_in_bytes, tensor_sizes_in_bytes);

#ifdef ENABLE_NVTX_PROFILE
 profile::NvtxRangeCreator postRange(
   "Batch-" + tag +
     " PostSend-" + std::to_string(dst), profile::Color::Red);
 
 postRange.Begin();
#endif

 
 Tensor* output_signal_tensor = ctx->Output(0, {});
 bool* output_signal = output_signal_tensor->MutableData<bool>();
 *output_signal = true;

#ifdef ENABLE_NVTX_PROFILE
 
 postRange.End();
#endif

 return Status::OK();
}

} 
} 

#endif
