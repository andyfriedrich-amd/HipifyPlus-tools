



#pragma once
#include "core/providers/cuda/cu_inc/common.cuh"

namespace onnxruntime {
namespace cuda {

inline int log2_ceil(int value) {
 int log2_value = 0;
 while ((1 << log2_value) < value) ++log2_value;
 return log2_value;
}

template <typename T>
struct Add {
 __device__ __forceinline__ T operator()(T a, T b) const {
  return a + b;
 }
};

template <typename T>
struct Max {
 __device__ __forceinline__ T operator()(T a, T b) const {
  return a < b ? b : a;
 }
};

template <typename acc_t, int WARP_BATCH, int WARP_SIZE, template <typename> class ReduceOp>
__device__ __forceinline__ void warp_reduce(acc_t* sum) {
 ReduceOp<acc_t> r;
#pragma unroll
 for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
#pragma unroll
  for (int i = 0; i < WARP_BATCH; ++i) {
   acc_t b = WARP_SHFL_XOR(sum[i], offset, WARP_SIZE);
   sum[i] = r(sum[i], b);
  }
 }
}



















template <typename input_t, typename output_t, typename acc_t, int log2_elements, bool is_log_softmax>
__global__ void softmax_warp_forward(output_t* dst, const input_t* src, int batch_size, int stride, int element_count) {
 
 constexpr int next_power_of_two = 1 << log2_elements;
 constexpr int WARP_SIZE = (next_power_of_two < GPU_WARP_SIZE) ? next_power_of_two : GPU_WARP_SIZE;
 constexpr int WARP_ITERATIONS = next_power_of_two / WARP_SIZE;
 constexpr int WARP_BATCH = (next_power_of_two <= 128) ? 2 : 1;

 int first_batch = (blockDim.y * blockIdx.x + threadIdx.y) * WARP_BATCH;

 
 
 int local_batches = batch_size - first_batch;
 if (local_batches > WARP_BATCH)
  local_batches = WARP_BATCH;

 
 int local_idx = threadIdx.x;

 src += first_batch * stride + local_idx;
 dst += first_batch * stride + local_idx;

 
 
 
 

 
 acc_t elements[WARP_BATCH][WARP_ITERATIONS];
 for (int i = 0; i < WARP_BATCH; ++i) {
  int batch_element_count = (i >= local_batches) ? 0 : element_count;
  for (int it = 0; it < WARP_ITERATIONS; ++it) {
   int element_index = local_idx + it * WARP_SIZE;
   if (element_index < batch_element_count) {
    elements[i][it] = src[i * element_count + it * WARP_SIZE];
   } else {
    elements[i][it] = -std::numeric_limits<acc_t>::infinity();
   }
  }
 }

 
 acc_t max_value[WARP_BATCH];
#pragma unroll
 for (int i = 0; i < WARP_BATCH; ++i) {
  max_value[i] = elements[i][0];
#pragma unroll
  for (int it = 1; it < WARP_ITERATIONS; ++it) {
   max_value[i] = (max_value[i] > elements[i][it]) ? max_value[i] : elements[i][it];
  }
 }
 warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Max>(max_value);

 acc_t sum[WARP_BATCH]{0.0f};
#pragma unroll
 for (int i = 0; i < WARP_BATCH; ++i) {
#pragma unroll
  for (int it = 0; it < WARP_ITERATIONS; ++it) {
   if (is_log_softmax) {
    sum[i] += std::exp((float)(elements[i][it] - max_value[i]));
   } else {
    elements[i][it] = std::exp((float)(elements[i][it] - max_value[i]));
    sum[i] += elements[i][it];
   }
  }
 }
 warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);


#pragma unroll
 for (int i = 0; i < WARP_BATCH; ++i) {
  if (i >= local_batches)
   break;
  if (is_log_softmax) sum[i] = max_value[i] + std::log((float)(sum[i]));
#pragma unroll
  for (int it = 0; it < WARP_ITERATIONS; ++it) {
   int element_index = local_idx + it * WARP_SIZE;
   if (element_index < element_count) {
    if (is_log_softmax) {
     dst[i * element_count + it * WARP_SIZE] = elements[i][it] - sum[i];
    } else {
     dst[i * element_count + it * WARP_SIZE] = elements[i][it] / sum[i];
    }
   } else {
    break;
   }
  }
 }
}






template <typename input_t, typename output_t, typename acc_t, int log2_elements, bool is_log_softmax>
__global__ void softmax_warp_forward_resource_efficient(output_t* dst, const input_t* src, int batch_size, int stride, int element_count) {
 
 constexpr int next_power_of_two = 1 << log2_elements;
 constexpr int WARP_SIZE = (next_power_of_two < GPU_WARP_SIZE) ? next_power_of_two : GPU_WARP_SIZE;
 constexpr int WARP_ITERATIONS = next_power_of_two / WARP_SIZE;

 int local_idx = threadIdx.x;
 src += blockIdx.x * stride + local_idx;
 dst += blockIdx.x * stride + local_idx;
 extern __shared__ unsigned char smem[];
 input_t (&elements)[WARP_ITERATIONS][WARP_SIZE] = *reinterpret_cast<input_t (*)[WARP_ITERATIONS][WARP_SIZE]>(smem);
#pragma unroll
 for (int it = 0; it < WARP_ITERATIONS; ++it) {
  int element_index = local_idx + it * WARP_SIZE;
  if (element_index < element_count) {
   elements[it][local_idx] = src[it * WARP_SIZE];
  } else {
   elements[it][local_idx] = -std::numeric_limits<input_t>::infinity();
  }
 }
 
 input_t max_value = elements[0][local_idx];
#pragma unroll
 for (int it = 1; it < WARP_ITERATIONS; ++it) {
  max_value = (max_value > elements[it][local_idx]) ? max_value : elements[it][local_idx];
 }
 warp_reduce<input_t, 1, WARP_SIZE, Max>(&max_value);
 
 acc_t sum{0.0f};
 
 
 for (int it = 0; it < WARP_ITERATIONS; ++it) {
  int element_index = local_idx + it * WARP_SIZE;
  if (element_index >= element_count)
   break;
  if (is_log_softmax) {
   sum += std::exp((float)(elements[it][local_idx] - max_value));
  } else {
   acc_t tmp = std::exp((float)(elements[it][local_idx] - max_value));
   elements[it][local_idx] = tmp;
   sum += tmp;
  }
 }
 warp_reduce<acc_t, 1, WARP_SIZE, Add>(&sum);
 
 if (is_log_softmax) sum = static_cast<acc_t>(max_value) + std::log((float)(sum));
 
 acc_t invsum = static_cast<acc_t>(1.0f / sum);
#pragma unroll
 for (int it = 0; it < WARP_ITERATIONS; ++it) {
  int element_index = local_idx + it * WARP_SIZE;
  if (element_index < element_count) {
   if (is_log_softmax) {
    dst[it * WARP_SIZE] = (float)elements[it][local_idx] - sum;
   } else {
    dst[it * WARP_SIZE] = (float)elements[it][local_idx] * invsum;
   }
  } else {
   break;
  }
 }
}

} 
} 
