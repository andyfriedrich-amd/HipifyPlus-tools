


#include "orttraining/training_ops/cuda/optimizer/adam_impl.h"
#include "core/providers/cuda/cuda_common.h"
#include "core/providers/cuda/cu_inc/common.cuh"
#include "orttraining/training_ops/cuda/optimizer/common.cuh"
#include "orttraining/training_ops/cpu/optimizer/common.h"
#include "orttraining/training_ops/cuda/optimizer/common.h"

namespace onnxruntime {
namespace cuda {
template <typename T1, typename T3, typename T4, typename T_GRAD, typename T_GRAD_NORM, typename T_MIXED_PRECISION_FP>
__global__ void _AdamOptimizer_mode0(
  const T1* eta, const T3* weights, const T_GRAD* grads, const T4* moment_1, const T4* moment_2, const T3* loss_scale, const T_GRAD_NORM* grad_norm, const float alpha, const float beta, const float lambda, const float epsilon, const float max_norm, const float alpha_correction, const float beta_correction, T4* moment_1_out, T4* moment_2_out, T3* weights_out, T_GRAD* grads_out, T_MIXED_PRECISION_FP* mixed_precision_weights_out, CUDA_LONG N) {
 CALCULATE_ELEMENTWISE_INDEX_OR_EXIT(id, N);
 const float actual_scale = _ComputeGradScale<T3, T_GRAD_NORM, float>(loss_scale, grad_norm, max_norm);

 
 const float g = static_cast<float>(grads[id]) / actual_scale;
 
 const float one = 1.0f;

 
 const float m1o = alpha * static_cast<float>(moment_1[id]) + (one - alpha) * g;
 const float m1o_corrected = m1o / alpha_correction;

 
 const float m2o = beta * static_cast<float>(moment_2[id]) + (one - beta) * g * g;
 const float m2o_corrected = m2o / beta_correction;

 
 const float denom = _Sqrt(m2o_corrected) + epsilon;
 const float update = (m1o_corrected / denom) + (lambda * weights[id]);

 const float delta = -static_cast<float>(*eta) * update;

 
 if (grads_out) {
  grads_out[id] = T_GRAD(delta);
 }

 
 if (weights_out) {
  weights_out[id] = weights[id] + T3(delta);

  if (mixed_precision_weights_out) {
   mixed_precision_weights_out[id] = static_cast<T_MIXED_PRECISION_FP>(weights_out[id]);
  }
 }

 moment_1_out[id] = m1o;
 moment_2_out[id] = m2o;
}

template <typename T1, typename T3, typename T4, typename T_GRAD, typename T_GRAD_NORM, typename T_MIXED_PRECISION_FP>
__global__ void _AdamOptimizer_mode1(
  const T1* eta, const T3* weights, const T_GRAD* grads, const T4* moment_1, const T4* moment_2, const T3* loss_scale, const T_GRAD_NORM* grad_norm, const float alpha, const float beta, const float lambda, const float epsilon, const float max_norm, const float alpha_correction, const float beta_correction, T4* moment_1_out, T4* moment_2_out, T3* weights_out, T_GRAD* grads_out, T_MIXED_PRECISION_FP* mixed_precision_weights_out, CUDA_LONG N) {
 CALCULATE_ELEMENTWISE_INDEX_OR_EXIT(id, N);
 const float actual_scale = _ComputeGradScale<T3, T_GRAD_NORM, float>(loss_scale, grad_norm, max_norm);

 
 const float g = static_cast<float>(grads[id]) / actual_scale;
 
 const float one = 1.0f;

 
 const float m1o = alpha * static_cast<float>(moment_1[id]) + (one - alpha) * g;

 
 const float m2o = beta * static_cast<float>(moment_2[id]) + (one - beta) * g * g;

 const float denom = _Sqrt(m2o) + epsilon;

 
 const float step_size = static_cast<float>(*eta) * _Sqrt(beta_correction) / alpha_correction;

 
 
 
 
 
 const float delta = -step_size * m1o / denom - static_cast<float>(*eta) * lambda * (weights[id] - step_size * m1o / denom);

 
 if (grads_out) {
  grads_out[id] = T_GRAD(delta);
 }

 
 if (weights_out) {
  weights_out[id] = weights[id] + T3(delta);

  if (mixed_precision_weights_out) {
   mixed_precision_weights_out[id] = static_cast<T_MIXED_PRECISION_FP>(weights_out[id]);
  }
 }

 moment_1_out[id] = m1o;
 moment_2_out[id] = m2o;
}

template <typename T1, typename T2, typename T3, typename T4, typename T_GRAD, typename T_GRAD_NORM, typename T_MIXED_PRECISION_FP>
void AdamOptimizerImpl(
  cudaStream_t stream, const T1* eta, const T2 update_count, const T3* weights, const T_GRAD* grads, const T4* moment_1, const T4* moment_2, const T3* loss_scale, const T_GRAD_NORM* grad_norm, const float alpha, const float beta, const float lambda, const float epsilon, const float max_norm, const bool do_bias_correction, const int64_t weight_decay_mode, T4* moment_1_out, T4* moment_2_out, T3* weights_out, T_GRAD* grads_out, T_MIXED_PRECISION_FP* mixed_precision_weights_out, size_t count) {
 int blocksPerGrid = (int)(ceil(static_cast<float>(count) / GridDim::maxThreadsPerBlock));
 CUDA_LONG N = static_cast<CUDA_LONG>(count);
 
 const float alpha_correction = do_bias_correction
                   ? contrib::compute_bias_correction_coefficient(alpha, update_count)
                   : 1.f;
 const float beta_correction = do_bias_correction
                  ? contrib::compute_bias_correction_coefficient(beta, update_count)
                  : 1.f;

 
 
 
 
 
 
 
 if (weight_decay_mode == 0) {
  _AdamOptimizer_mode0<T1, T3, T4, T_GRAD, T_GRAD_NORM, T_MIXED_PRECISION_FP><<<blocksPerGrid, GridDim::maxThreadsPerBlock, 0, stream>>>(
    eta, weights, grads, moment_1, moment_2, loss_scale, grad_norm, alpha, beta, lambda, epsilon, max_norm, alpha_correction, beta_correction, moment_1_out, moment_2_out, weights_out, grads_out, mixed_precision_weights_out, N);
 } else if (weight_decay_mode == 1) {
  _AdamOptimizer_mode1<T1, T3, T4, T_GRAD, T_GRAD_NORM, T_MIXED_PRECISION_FP><<<blocksPerGrid, GridDim::maxThreadsPerBlock, 0, stream>>>(
    eta, weights, grads, moment_1, moment_2, loss_scale, grad_norm, alpha, beta, lambda, epsilon, max_norm, alpha_correction, beta_correction, moment_1_out, moment_2_out, weights_out, grads_out, mixed_precision_weights_out, N);
 } else {
  
  ORT_THROW("Unsupported Adamw optimizer mode.");
 }
}

#define SPECIALIZED_AdamOptimizerImpl(T1, T2, T3, T4, T_GRAD, T_GRAD_NORM, T_MIXED_PRECISION_FP)  template void AdamOptimizerImpl(                                   cudaStream_t stream, const T1* eta, const T2 update_count, const T3* weights, const T_GRAD* grads, const T4* moment_1, const T4* moment_2, const T3* loss_scale, const T_GRAD_NORM* grad_norm, const float alpha, const float beta, const float lambda, const float epsilon, const float max_norm, const bool do_bias_correction, const int64_t weight_decay_mode, T4* moment_1_out, T4* moment_2_out, T3* weights_out, T_GRAD* grads_out, T_MIXED_PRECISION_FP* mixed_precision_weights_out, size_t count);

SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, float, float, float, half);
SPECIALIZED_AdamOptimizerImpl(half, int64_t, float, half, float, float, half);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, half, float, float, half);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, float, half, half, half);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, float, half, float, half);
SPECIALIZED_AdamOptimizerImpl(half, int64_t, float, half, half, half, half);
SPECIALIZED_AdamOptimizerImpl(half, int64_t, float, half, half, float, half);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, half, half, half, half);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, half, half, float, half);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, float, float, float, BFloat16);
SPECIALIZED_AdamOptimizerImpl(BFloat16, int64_t, float, BFloat16, float, float, BFloat16);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, BFloat16, float, float, BFloat16);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, float, BFloat16, BFloat16, BFloat16);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, float, BFloat16, float, BFloat16);
SPECIALIZED_AdamOptimizerImpl(BFloat16, int64_t, float, BFloat16, BFloat16, BFloat16, BFloat16);
SPECIALIZED_AdamOptimizerImpl(BFloat16, int64_t, float, BFloat16, BFloat16, float, BFloat16);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, BFloat16, BFloat16, BFloat16, BFloat16);
SPECIALIZED_AdamOptimizerImpl(float, int64_t, float, BFloat16, BFloat16, float, BFloat16);

} 
} 
