


#include <cuda_fp16.h>
#include <cuda_runtime_api.h>
#include <cub/cub.cuh>
#include "core/providers/cuda/cuda_common.h"
#include "contrib_ops/cuda/diffusion/group_norm_impl.h"
#include "contrib_ops/cuda/transformers/dump_cuda_tensor.h"

namespace onnxruntime {
namespace contrib {
namespace cuda {

static inline int32_t divUp(int32_t m, int32_t n) {
 return (m + n - 1) / n;
}

static inline __device__ __host__ float sigmoid(float x) {
 return 1.F / (1.F + expf(-x));
}

struct GroupSums {
 
 int32_t flag;
 
 float sum;
 
 float sumSq;
};

struct GroupSumsOp {
 inline __device__ GroupSums operator()(GroupSums const& a, GroupSums const& b) {
  GroupSums dst;
  dst.sum = b.flag ? b.sum : (a.sum + b.sum);
  dst.sumSq = b.flag ? b.sumSq : (a.sumSq + b.sumSq);
  dst.flag = a.flag + b.flag;
  return dst;
 }
};

template <typename T>
struct GroupNormNHWCParams {
 
 T* dst;
 
 T const* src;
 
 float const* gamma;
 
 float const* beta;
 
 
 float* redBuffer;

 
 int32_t n;
 
 int32_t h;
 int32_t w;
 
 int32_t c;
 
 int32_t groups;
 
 bool withSwish;

 

 
 
 int32_t hw;
 int32_t hwPerBlock;
 
 
 int32_t cPerBlock;
 int32_t cPerGroup;

 
 int32_t hwc;
 
 float invHWC;
 
 int32_t groupsPerBlock;
};

template <typename T>
inline __device__ void UpdateSum(const T* src, int64_t offset, float& sum, float& sumSq);

template <>
inline __device__ void UpdateSum(const half* src, int64_t offset, float& sum, float& sumSq) {
 
 __half2 h2 = *reinterpret_cast<__half2 const*>(&src[offset]);

 float2 f2 = __half22float2(h2);

 
 sum += f2.x + f2.y;

 
 sumSq += f2.x * f2.x + f2.y * f2.y;
}

template <>
inline __device__ void UpdateSum(const float* src, int64_t offset, float& sum, float& sumSq) {
 
 float2 f2 = *reinterpret_cast<float2 const*>(&src[offset]);

 
 sum += f2.x + f2.y;

 
 sumSq += f2.x * f2.x + f2.y * f2.y;
}

template <typename T, int32_t tTHREADS_PER_BLOCK>
__global__ void groupNormNHWCSumKernel(GroupNormNHWCParams<T> params) {
 
 typedef cub::BlockScan<GroupSums, tTHREADS_PER_BLOCK> BlockScan;

 
 __shared__ typename BlockScan::TempStorage tempStorage;
 
 
 __shared__ float2 smem[tTHREADS_PER_BLOCK];

 
 int32_t ni = blockIdx.z;
 
 int32_t ci = blockIdx.x * params.cPerBlock + threadIdx.x * 2;

 
 int32_t hwBegin = blockIdx.y * params.hwPerBlock;
 
 int32_t hwEnd = min(hwBegin + params.hwPerBlock, params.hw);

 
 float sum = 0.F;
 float sumSq = 0.F;

 
 if (ci < params.c) {
  for (int32_t hwi = hwBegin; hwi < hwEnd; ++hwi) {
   
   int64_t offset = static_cast<int64_t>(ni) * params.hwc + static_cast<int64_t>(hwi) * params.c + ci;
   UpdateSum(params.src, offset, sum, sumSq);
  }
 }

 
 int32_t gi = threadIdx.x * 2 / params.cPerGroup;
 int32_t cj = threadIdx.x * 2 - params.cPerGroup * gi;

 
 GroupSums inp{cj == 0 ? 1 : 0, sum, sumSq};

 
 GroupSums out;
 BlockScan(tempStorage).InclusiveScan(inp, out, GroupSumsOp());

 
 
 if (cj == params.cPerGroup - 2) { 
  smem[gi] = make_float2(out.sum, out.sumSq);
 }

 
 __syncthreads();

 
 int32_t gj = blockIdx.x * params.groupsPerBlock + threadIdx.x;

 
 if (threadIdx.x >= params.groupsPerBlock || gj >= params.groups) {
  return;
 }

 
 float2 sums = smem[threadIdx.x];

 
 atomicAdd(&params.redBuffer[(2 * ni + 0) * params.groups + gj], sums.x);
 atomicAdd(&params.redBuffer[(2 * ni + 1) * params.groups + gj], sums.y);
}

template <typename T>
void groupNormNHWCSum(GroupNormNHWCParams<T> const& params, cudaStream_t stream) {
 
 ORT_ENFORCE(params.c % params.cPerBlock == 0 && params.hw % params.hwPerBlock == 0);
 
 ORT_ENFORCE(params.cPerBlock % params.cPerGroup == 0);

 dim3 grid;

 
 grid.x = params.c / params.cPerBlock;
 
 grid.y = divUp(params.hw, params.hwPerBlock);
 
 grid.z = params.n;

 switch (params.cPerBlock) {
  case 320:
   groupNormNHWCSumKernel<T, 160><<<grid, 160, 0, stream>>>(params);
   break;
  case 480:
   groupNormNHWCSumKernel<T, 256><<<grid, 256, 0, stream>>>(params);
   break;
  case 256:
   groupNormNHWCSumKernel<T, 128><<<grid, 128, 0, stream>>>(params);
   break;
  case 128:
   groupNormNHWCSumKernel<T, 64><<<grid, 64, 0, stream>>>(params);
   break;
  default:
   ORT_NOT_IMPLEMENTED("Not implemented");
 }
}

template <typename T>
__device__ void computeGroupNorm(const T* src, T* dst, int64_t offset, float mean, float invStdDev, float2& gammaF2, float2& betaF2, bool swish);

template <>
__device__ void computeGroupNorm(const half* src, half* dst, int64_t offset, float mean, float invStdDev, float2& gammaF2, float2& betaF2, bool swish) {
 
 __half2 h2 = *reinterpret_cast<__half2 const*>(&src[offset]);

 
 float2 f2 = __half22float2(h2);

 
 f2.x = (f2.x - mean) * invStdDev;
 f2.y = (f2.y - mean) * invStdDev;

 
 f2.x = gammaF2.x * f2.x + betaF2.x;
 f2.y = gammaF2.y * f2.y + betaF2.y;

 
 if (swish) {
  f2.x = f2.x * sigmoid(f2.x);
  f2.y = f2.y * sigmoid(f2.y);
 }

 *reinterpret_cast<__half2*>(&dst[offset]) = __float22half2_rn(f2);
}

template <>
__device__ void computeGroupNorm(const float* src, float* dst, int64_t offset, float mean, float invStdDev, float2& gammaF2, float2& betaF2, bool swish) {
 
 float2 f2 = *reinterpret_cast<float2 const*>(&src[offset]);

 
 f2.x = (f2.x - mean) * invStdDev;
 f2.y = (f2.y - mean) * invStdDev;

 
 f2.x = gammaF2.x * f2.x + betaF2.x;
 f2.y = gammaF2.y * f2.y + betaF2.y;

 
 if (swish) {
  f2.x = f2.x * sigmoid(f2.x);
  f2.y = f2.y * sigmoid(f2.y);
 }

 *reinterpret_cast<float2*>(&dst[offset]) = f2;
}

template <typename T, int32_t tTHREADS_PER_BLOCK>
__global__ void groupNormNHWCScaleKernel(GroupNormNHWCParams<T> params) {
 
 int32_t ci = blockIdx.x * params.cPerBlock + threadIdx.x * 2;
 if (ci >= params.c) {
  return;
 }

 
 int32_t ni = blockIdx.z;

 
 int32_t gi = ci / params.cPerGroup;

 
 float sum = 0.F, sumSq = 0.F;
 if (gi < params.groups) {
  sum = params.redBuffer[(2 * ni + 0) * params.groups + gi];
  sumSq = params.redBuffer[(2 * ni + 1) * params.groups + gi];
 }

 
 float2 gammaF2 = *reinterpret_cast<float2 const*>(&params.gamma[ci]);
 float2 betaF2 = *reinterpret_cast<float2 const*>(&params.beta[ci]);

 
 float mean = sum * params.invHWC;
 
 float var = sumSq * params.invHWC - (mean * mean);
 
 float invStdDev = var <= 0.F ? 1.F : rsqrtf(var);

 
 int32_t hwBegin = blockIdx.y * params.hwPerBlock;
 
 int32_t hwEnd = min(hwBegin + params.hwPerBlock, params.hw);

 
 for (int32_t hwi = hwBegin; hwi < hwEnd; ++hwi) {
  
  int64_t offset = (int64_t)ni * params.hwc + hwi * params.c + ci;

  
  computeGroupNorm<T>(params.src, params.dst, offset, mean, invStdDev, gammaF2, betaF2, params.withSwish);
 }
}

template <typename T>
void groupNormNHWCScale(GroupNormNHWCParams<T> const& params, cudaStream_t stream) {
 
 ORT_ENFORCE(params.c % params.cPerBlock == 0);
 
 ORT_ENFORCE(params.cPerBlock % params.cPerGroup == 0);

 dim3 grid;

 
 grid.x = params.c / params.cPerBlock;
 
 grid.y = divUp(params.hw, params.hwPerBlock);
 
 grid.z = params.n;

 switch (params.cPerBlock) {
  case 320:
   groupNormNHWCScaleKernel<T, 160><<<grid, 160, 0, stream>>>(params);
   break;
  case 480:
   groupNormNHWCScaleKernel<T, 256><<<grid, 256, 0, stream>>>(params);
   break;
  case 256:
   groupNormNHWCScaleKernel<T, 128><<<grid, 128, 0, stream>>>(params);
   break;
  case 128:
   groupNormNHWCScaleKernel<T, 64><<<grid, 64, 0, stream>>>(params);
   break;
  default:
   ORT_NOT_IMPLEMENTED("Not implemented");
 }
}

int32_t findMaxDivisor(int32_t n, int32_t maxAllowedDivisor) {
 int32_t maxDivisor = -1;
 for (int32_t i = 1; i <= std::sqrt(n); i++) {
  if (n % i == 0) {
   int32_t divisor1 = n / i;
   int32_t divisor2 = i;

   if (divisor1 > maxDivisor && divisor1 < maxAllowedDivisor) {
    maxDivisor = divisor1;
   }
   if (divisor2 > maxDivisor && divisor2 < maxAllowedDivisor) {
    maxDivisor = divisor2;
   }
  }
 }
 return maxDivisor;
}

template <typename T>
Status LaunchGroupNormKernel(
  cudaStream_t stream, T* output, const T* input, const float* gamma, const float* beta, void* workspace, float epsilon, int batch_size, int num_channels, int height, int width, int num_groups, bool use_swish_activation) {
 if (batch_size > static_cast<int>(kMaxGroupNormBatchSize)) {
  return ORT_MAKE_STATUS(ONNXRUNTIME, StatusCode::NOT_IMPLEMENTED, "only support batch_size <= 32. Got", batch_size);
 }

 if (num_groups != static_cast<int>(kGroupNormNumberOfGroups)) {
  return ORT_MAKE_STATUS(ONNXRUNTIME, StatusCode::NOT_IMPLEMENTED, "only num_groups=32 is supported. Got", num_groups);
 }

 GroupNormNHWCParams<T> params;
 int32_t cPerBlock = 320;
 int32_t maxBlocksPerHW = 1024;
 switch (num_channels) {
  case 960:
  case 1920:
   cPerBlock = 480;
   break;
  case 512:
  case 256:
   cPerBlock = 256;
   break;
  case 128:
   cPerBlock = 128;
   break;
  default:
   cPerBlock = 320;
 }

 params.withSwish = use_swish_activation;
 params.dst = output;
 params.src = input;
 params.gamma = gamma;
 params.beta = beta;
 params.redBuffer = reinterpret_cast<float*>(workspace);
 params.n = batch_size;
 params.h = height;
 params.w = width;
 params.c = num_channels;
 params.groups = num_groups;
 params.hw = params.h * params.w;
 const int32_t blocksPerHW = findMaxDivisor(params.hw, maxBlocksPerHW);
 params.hwPerBlock = divUp(params.hw, blocksPerHW);
 params.cPerBlock = cPerBlock;
 params.cPerGroup = params.c / params.groups;
 params.hwc = params.hw * params.c;
 params.invHWC = 1.F / (float)(params.hw * params.cPerGroup);
 params.groupsPerBlock = cPerBlock / params.cPerGroup;

 DUMP_TENSOR_INIT();
 DUMP_TENSOR("input", input, batch_size, num_channels, height * width);
 DUMP_TENSOR("gamma", gamma, 1, num_channels);
 DUMP_TENSOR("beta", beta, 1, num_channels);
 cudaMemsetAsync(params.redBuffer, 0, GetGroupNormWorkspaceSizeInBytes(), stream);
 groupNormNHWCSum<T>(params, stream);
 DUMP_TENSOR("workspace", params.redBuffer, batch_size, num_groups, 2);
 CUDA_RETURN_IF_ERROR(cudaGetLastError());
 groupNormNHWCScale<T>(params, stream);
 CUDA_RETURN_IF_ERROR(cudaGetLastError());
 DUMP_TENSOR("output", output, batch_size, num_channels, height * width);
 return Status::OK();
}

template Status LaunchGroupNormKernel<half>(cudaStream_t stream, half* output, const half* input, const float* gamma, const float* beta, void* workspace, float epsilon, int batch_size, int num_channels, int height, int width, int num_groups, bool swish);

template Status LaunchGroupNormKernel<float>(cudaStream_t stream, float* output, const float* input, const float* gamma, const float* beta, void* workspace, float epsilon, int batch_size, int num_channels, int height, int width, int num_groups, bool swish);
} 
} 
} 
