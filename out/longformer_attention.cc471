


#include "core/providers/rocm/shared_inc/fpgeneric.h"
#include "core/platform/env_var_utils.h"
#include "contrib_ops/rocm/bert/longformer_global_impl.h"
#include "contrib_ops/rocm/bert/longformer_attention_impl.h"
#include "contrib_ops/rocm/bert/transformer_rocm_common.h"
#include "contrib_ops/rocm/bert/longformer_attention.h"

using namespace onnxruntime::rocm;
using namespace ::onnxruntime::common;
using namespace ONNX_NAMESPACE;

namespace onnxruntime {
namespace contrib {
namespace rocm {

#define REGISTER_KERNEL_TYPED(T)                   ONNX_OPERATOR_TYPED_KERNEL_EX(                     LongformerAttention, kMSDomain, 1, T, kRocmExecutionProvider, (*KernelDefBuilder::Create())                     .TypeConstraint("T", DataTypeImpl::GetTensorType<T>()), LongformerAttention<T>);

REGISTER_KERNEL_TYPED(float)
REGISTER_KERNEL_TYPED(MLFloat16)

template <typename T>
LongformerAttention<T>::LongformerAttention(const OpKernelInfo& info)
  : RocmKernel(info), LongformerAttentionBase(info) {
 use_compact_memory_ = ParseEnvironmentVariableWithDefault<bool>(longformer::kUseCompactMemory, true);
 use_half4_ = ParseEnvironmentVariableWithDefault<bool>(longformer::kUseHalf4, true);
}

template <typename T>
Status LongformerAttention<T>::ComputeInternal(OpKernelContext* context) const {
 const Tensor* input = context->Input<Tensor>(0);
 const Tensor* weights = context->Input<Tensor>(1);
 const Tensor* bias = context->Input<Tensor>(2);
 const Tensor* attention_mask = context->Input<Tensor>(3);
 const Tensor* global_weights = context->Input<Tensor>(4);
 const Tensor* global_bias = context->Input<Tensor>(5);
 const Tensor* global_attention_mask = context->Input<Tensor>(6);
 ORT_RETURN_IF_ERROR(CheckInputs(input->Shape(), weights->Shape(), bias->Shape(), attention_mask->Shape(), global_weights->Shape(), global_bias->Shape(), global_attention_mask->Shape()));
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 const auto& shape = input->Shape();
 int batch_size = static_cast<int>(shape[0]);
 int sequence_length = static_cast<int>(shape[1]);
 int hidden_size = static_cast<int>(shape[2]);
 int head_size = hidden_size / num_heads_;

 Tensor* output = context->Output(0, shape);

 rocblas_handle rocblas = GetRocblasHandle(context);
 hipStream_t stream = Stream(context);

 constexpr size_t element_size = sizeof(T);

 
 
 auto global_index_buffer = GetScratchBuffer<int>(static_cast<size_t>(batch_size) * sequence_length, context->GetComputeStream());
 auto batch_global_num_buffer = GetScratchBuffer<int>(batch_size, context->GetComputeStream());

 size_t global_scratch_bytes = GetGlobalScratchSize(sequence_length);
 auto global_scratch_buffer = GetScratchBuffer<void>(global_scratch_bytes, context->GetComputeStream());

 auto& device_prop = GetDeviceProp();
 ORT_RETURN_IF_ERROR(BuildGlobalIndex(
   device_prop, stream, global_attention_mask->Data<int>(), batch_size, sequence_length, global_index_buffer.get(), batch_global_num_buffer.get(), global_scratch_buffer.get(), global_scratch_bytes));

 
 size_t pinned_buffer_bytes = GetPinnedBufferSize(batch_size);
 auto pinned_buffer = AllocateBufferOnCPUPinned<void>(pinned_buffer_bytes);
 int* batch_global_num_pinned = reinterpret_cast<int*>(pinned_buffer.get());
 HIP_RETURN_IF_ERROR(hipMemcpyAsync(batch_global_num_pinned, batch_global_num_buffer.get(), batch_size * sizeof(int), hipMemcpyDeviceToHost, stream));

 
 AutoDestoryCudaEvent new_event;
 hipEvent_t& is_copy_done = new_event.Get();

 HIP_RETURN_IF_ERROR(hipEventCreateWithFlags(&is_copy_done, hipEventDisableTiming));
 HIP_RETURN_IF_ERROR(hipEventRecord(is_copy_done, stream));

 size_t qkv_size = static_cast<size_t>(batch_size) * sequence_length * 3 * hidden_size * element_size;
 
 
 auto gemm_buffer = GetScratchBuffer<void>(qkv_size + qkv_size, context->GetComputeStream());

 bool use_merged_qkv_weights = (weights->Shape().NumDimensions() == 2);

 int m = batch_size * sequence_length;
 int n = use_merged_qkv_weights ? 3 * hidden_size : hidden_size;
 int k = hidden_size;
 typedef typename ToHipType<T>::MappedType HipT;
 const HipT* input_data = reinterpret_cast<const HipT*>(input->Data<T>());
 const HipT* weights_data = reinterpret_cast<const HipT*>(weights->Data<T>());
 const HipT* global_weights_data = reinterpret_cast<const HipT*>(global_weights->Data<T>());

 float one = 1.0f;
 float zero = 0.0f;
 if (use_merged_qkv_weights) {
  
  ROCBLAS_RETURN_IF_ERROR(rocblasGemmHelper(
    rocblas, rocblas_operation_none, rocblas_operation_none, n, m, k, &one, weights_data, n, input_data, k, &zero, reinterpret_cast<HipT*>(gemm_buffer.get()), n, device_prop));
 } else {
  
  const HipT* q_weight = weights_data;
  HipT* q_data = reinterpret_cast<HipT*>(gemm_buffer.get());
  ROCBLAS_RETURN_IF_ERROR(rocblasGemmHelper(
    rocblas, rocblas_operation_none, rocblas_operation_none, n, m, k, &one, q_weight, n, input_data, k, &zero, q_data, n, device_prop));
  
  const HipT* k_weight = q_weight + static_cast<int64_t>(hidden_size) * hidden_size;
  HipT* k_data = q_data + static_cast<int64_t>(batch_size) * sequence_length * hidden_size;
  ROCBLAS_RETURN_IF_ERROR(rocblasGemmHelper(
    rocblas, rocblas_operation_none, rocblas_operation_none, n, m, k, &one, k_weight, n, input_data, k, &zero, k_data, n, device_prop));

  
  const HipT* v_weight = k_weight + static_cast<int64_t>(hidden_size) * hidden_size;
  HipT* v_data = k_data + static_cast<int64_t>(batch_size) * sequence_length * hidden_size;
  ROCBLAS_RETURN_IF_ERROR(rocblasGemmHelper(
    rocblas, rocblas_operation_none, rocblas_operation_none, n, m, k, &one, v_weight, n, input_data, k, &zero, v_data, n, device_prop));
 }

 
 HIP_RETURN_IF_ERROR(hipEventSynchronize(is_copy_done));

 
 int max_num_global = 0;
 for (int i = 0; i < batch_size; ++i) {
  if (max_num_global < batch_global_num_pinned[i]) {
   max_num_global = batch_global_num_pinned[i];
  }
 }

 
 
 
 
 bool disable_compact_memory = (max_num_global > window_ || sequence_length == 2 * window_ || !use_compact_memory_);

 
 
 
 HipT* global_gemm_buffer = nullptr;

 if (max_num_global > 0) {
  global_gemm_buffer = reinterpret_cast<HipT*>(reinterpret_cast<char*>(gemm_buffer.get()) + qkv_size);

  if (use_merged_qkv_weights) {
   ROCBLAS_RETURN_IF_ERROR(rocblasGemmHelper(
     rocblas, rocblas_operation_none, rocblas_operation_none, n, m, k, &one, reinterpret_cast<const HipT*>(global_weights->Data<T>()), n, input_data, k, &zero, global_gemm_buffer, n, device_prop));
  } else {
   
   const HipT* global_q_weight = global_weights_data;
   HipT* global_q = global_gemm_buffer + static_cast<int64_t>(2) * batch_size * sequence_length * hidden_size;
   if (disable_compact_memory) {
    ROCBLAS_RETURN_IF_ERROR(rocblasGemmHelper(
      rocblas, rocblas_operation_none, rocblas_operation_none, n, m, k, &one, global_q_weight, n, input_data, k, &zero, global_q, n, device_prop));
   } else {
    ROCBLAS_RETURN_IF_ERROR(rocblasGemmStridedBatchedHelper(
      rocblas, rocblas_operation_none, rocblas_operation_none, hidden_size, max_num_global, hidden_size, &one, global_q_weight, hidden_size, 0, input_data, hidden_size, static_cast<int64_t>(sequence_length) * hidden_size, &zero, global_q, hidden_size, static_cast<int64_t>(max_num_global) * hidden_size, batch_size, device_prop));
   }
   
   const HipT* global_k_weight = global_weights_data + static_cast<int64_t>(hidden_size) * hidden_size;
   HipT* global_k = global_gemm_buffer;
   ROCBLAS_RETURN_IF_ERROR(rocblasGemmHelper(
     rocblas, rocblas_operation_none, rocblas_operation_none, n, m, k, &one, global_k_weight, n, input_data, k, &zero, global_k, n, device_prop));

   
   const HipT* global_v_weight = global_k_weight + static_cast<int64_t>(hidden_size) * hidden_size;
   HipT* global_v = global_gemm_buffer + static_cast<int64_t>(batch_size) * sequence_length * hidden_size;
   ROCBLAS_RETURN_IF_ERROR(rocblasGemmHelper(
     rocblas, rocblas_operation_none, rocblas_operation_none, n, m, k, &one, global_v_weight, n, input_data, k, &zero, global_v, n, device_prop));
  }
 }

 size_t workSpaceSize = GetLongformerAttentionWorkspaceSize(element_size, batch_size, num_heads_, head_size, sequence_length, max_num_global, window_, disable_compact_memory);
 auto workspace_buffer = GetScratchBuffer<void>(workSpaceSize, context->GetComputeStream());
 ORT_RETURN_IF_ERROR(LaunchLongformerAttentionKernel(
   device_prop, rocblas, stream, reinterpret_cast<const HipT*>(gemm_buffer.get()), reinterpret_cast<const HipT*>(bias->Data<T>()), reinterpret_cast<const HipT*>(attention_mask->Data<T>()), reinterpret_cast<const HipT*>(global_gemm_buffer), reinterpret_cast<const HipT*>(global_bias->Data<T>()), global_attention_mask->Data<int>(), global_index_buffer.get(), batch_global_num_buffer.get(), pinned_buffer.get(), workspace_buffer.get(), output->MutableData<T>(), batch_size, sequence_length, num_heads_, head_size, window_, max_num_global, element_size, disable_compact_memory, use_merged_qkv_weights, use_half4_));

 
 this->AddDeferredReleaseCPUPtr(pinned_buffer.release(), context->GetComputeStream());

 return Status::OK();
}

} 
} 
} 
