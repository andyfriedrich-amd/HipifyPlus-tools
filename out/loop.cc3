


#include "core/providers/shared_library/provider_api.h"
#include "core/providers/cuda/controlflow/loop.h"
#include "core/providers/cuda/cuda_common.h"
#include "core/providers/cuda/cuda_fwd.h"
#include "core/providers/cuda/cuda_execution_provider.h"

using namespace ONNX_NAMESPACE;
using namespace onnxruntime::common;

namespace onnxruntime {
namespace cuda {

ONNX_OPERATOR_VERSIONED_KERNEL_EX(Loop, kOnnxDomain, 1, 10, kCudaExecutionProvider, (*KernelDefBuilder::Create())
                   .InputMemoryType(OrtMemTypeCPUInput, 0) 
                   .InputMemoryType(OrtMemTypeCPUInput, 1) 
                   .TypeConstraint("I", DataTypeImpl::GetTensorType<int64_t>())
                   .TypeConstraint("B", DataTypeImpl::GetTensorType<bool>())
                   .TypeConstraint("V", DataTypeImpl::AllFixedSizeTensorTypes()), Loop);


ONNX_OPERATOR_VERSIONED_KERNEL_EX(Loop, kOnnxDomain, 11, 12, kCudaExecutionProvider, (*KernelDefBuilder::Create())
                   .InputMemoryType(OrtMemTypeCPUInput, 0) 
                   .InputMemoryType(OrtMemTypeCPUInput, 1) 
                   .TypeConstraint("I", DataTypeImpl::GetTensorType<int64_t>())
                   .TypeConstraint("B", DataTypeImpl::GetTensorType<bool>())
                   .TypeConstraint("V", DataTypeImpl::AllFixedSizeTensorTypes()), Loop);


ONNX_OPERATOR_VERSIONED_KERNEL_EX(Loop, kOnnxDomain, 13, 18, kCudaExecutionProvider, (*KernelDefBuilder::Create())
                   .InputMemoryType(OrtMemTypeCPUInput, 0) 
                   .InputMemoryType(OrtMemTypeCPUInput, 1) 
                   .TypeConstraint("I", DataTypeImpl::GetTensorType<int64_t>())
                   .TypeConstraint("B", DataTypeImpl::GetTensorType<bool>())
                   .TypeConstraint("V", DataTypeImpl::AllTensorAndSequenceTensorTypes()), Loop);


ONNX_OPERATOR_KERNEL_EX(Loop, kOnnxDomain, 19, kCudaExecutionProvider, (*KernelDefBuilder::Create())
              .InputMemoryType(OrtMemTypeCPUInput, 0) 
              .InputMemoryType(OrtMemTypeCPUInput, 1) 
              .TypeConstraint("I", DataTypeImpl::GetTensorType<int64_t>())
              .TypeConstraint("B", DataTypeImpl::GetTensorType<bool>())
              .TypeConstraint("V", DataTypeImpl::AllTensorAndSequenceTensorTypesIRv9()), Loop);

static Status ConcatenateGpuOutput(void* stream, std::vector<OrtValue>& per_iteration_output, void* output, ptrdiff_t output_size_in_bytes) {
 const auto& first_output = per_iteration_output.front().Get<Tensor>();
 const auto& per_iteration_shape = first_output.Shape();
 size_t bytes_per_iteration = first_output.SizeInBytes();

 void* cur_output = output;
 for (size_t i = 0, num_iterations = per_iteration_output.size(); i < num_iterations; ++i) {
  auto& ort_value = per_iteration_output[i];
  auto& iteration_data = ort_value.Get<Tensor>();

  
  if (bytes_per_iteration != iteration_data.SizeInBytes()) {
   return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "Inconsistent shape in loop output for output. ", " Expected:", per_iteration_shape, " Got:", iteration_data.Shape());
  }

  CUDA_RETURN_IF_ERROR(cudaMemcpyAsync(cur_output, iteration_data.DataRaw(), bytes_per_iteration, cudaMemcpyDeviceToDevice, static_cast<cudaStream_t>(stream)));

  cur_output = static_cast<void*>((static_cast<gsl::byte*>(cur_output) + bytes_per_iteration));
 }

 ORT_ENFORCE(static_cast<gsl::byte*>(cur_output) - static_cast<gsl::byte*>(output) == output_size_in_bytes, "Concatenation did not fill output buffer as expected.");

 return Status::OK();
}

Loop::Loop(const OpKernelInfo& info) : onnxruntime::Loop(info) {
 
 
 
 
 
 
 
 
 
 
 
 bool do_copy_on_default_stream = static_cast<const CUDAExecutionProvider*>(info.GetExecutionProvider())->DoCopyOnDefaultStream();
 ORT_ENFORCE(do_copy_on_default_stream, "Using Loop operator on CUDA while using a dedicated stream for copying "
       "(a stream that is different than the compute stream) is currently not supported");
 SetConcatOutputFunc(ConcatenateGpuOutput);
}

Status Loop::Compute(OpKernelContext* ctx) const {
 
 
 
 
 
 auto status = onnxruntime::Loop::Compute(ctx);
 return status;
}

} 
} 
