


#include <memory>
#include <utility>

#include "orttraining/training_ops/rocm/optimizer/clip_grad_norm/clip_grad_norm.h"
#include "orttraining/training_ops/rocm/reduction/reduction_all_impl.h"
#include "orttraining/training_ops/rocm/optimizer/clip_grad_norm/clip_grad_norm_impl.h"

namespace onnxruntime {
namespace rocm {

namespace {

constexpr int ChunkSize = 2048 * 32;
constexpr float Epsilon = 0.000001f;

void GetGroupedTensors(const TensorSeq* gradients, InlinedVector<int>* tensor_sizes, InlinedVector<std::vector<void*>>* grouped_tensor_pointers) {
 for (size_t i = 0; i < gradients->Size(); ++i) {
  (*tensor_sizes)[i] = static_cast<int>(gradients->Get(i).Shape().Size());
  (*grouped_tensor_pointers)[i] = {const_cast<float*>(gradients->Get(i).Data<float>())};
 }
}

Status GetL2Norm(hipStream_t stream, InlinedVector<int>& tensor_sizes, InlinedVector<std::vector<void*>>& grouped_tensor_pointers, float** l2_norm) {
 HIP_RETURN_IF_ERROR(hipMemsetAsync(*l2_norm, 0, sizeof(float), stream));
 MultiTensorReduceL2<float, float> multi_tensor_reduce_l2_functor;
 launch_multi_tensor_functor<ClipGradNormGroupSize, MultiTensorReduceL2<float, float>>(
   stream, ChunkSize, tensor_sizes, grouped_tensor_pointers, multi_tensor_reduce_l2_functor, *l2_norm);

 ScalarSqrt(stream, *l2_norm, *l2_norm);

 return Status::OK();
}

Status PopulateOutput(hipStream_t stream, AllocatorPtr alloc, const TensorSeq* gradients, TensorSeq** clipped_gradients) {
 
 
 if (gradients == *clipped_gradients) {
  return Status::OK();
 }

 (*clipped_gradients)->SetType(gradients->DataType());
 (*clipped_gradients)->Reserve(gradients->Size());
 for (size_t gradient_idx = 0; gradient_idx < gradients->Size(); ++gradient_idx) {
  const Tensor& source_tensor = gradients->Get(gradient_idx);
  std::unique_ptr<Tensor> target_tensor = Tensor::Create(source_tensor.DataType(), source_tensor.Shape(), alloc);
  HIP_RETURN_IF_ERROR(hipMemcpyAsync(target_tensor->MutableDataRaw(), source_tensor.DataRaw(), source_tensor.SizeInBytes(), hipMemcpyDeviceToDevice, stream));
  (*clipped_gradients)->Add(std::move(*target_tensor)); 
 }

 return Status::OK();
}

} 

ONNX_OPERATOR_KERNEL_EX(
  InplaceClipGradNorm, kMSDomain, 1, kRocmExecutionProvider, (*KernelDefBuilder::Create())
    .Alias(0, 0) 
           
           
    .TypeConstraint("S_GRAD", DataTypeImpl::AllFixedSizeSequenceTensorTypes()), InplaceClipGradNorm);

Status InplaceClipGradNorm::ComputeInternal(OpKernelContext* ctx) const {
 
 const TensorSeq* gradients = ctx->Input<TensorSeq>(0);
 InlinedVector<int> tensor_sizes(gradients->Size());
 
 
 InlinedVector<std::vector<void*>> grouped_tensor_pointers(gradients->Size());
 GetGroupedTensors(gradients, &tensor_sizes, &grouped_tensor_pointers);

 AllocatorPtr alloc;
 ORT_RETURN_IF_ERROR(ctx->GetTempSpaceAllocator(&alloc));

 
 float* total_norm = reinterpret_cast<float*>(alloc->Alloc(sizeof(float)));
 ORT_RETURN_IF_ERROR(GetL2Norm(Stream(ctx), tensor_sizes, grouped_tensor_pointers, &total_norm));

 
 ClipGradNormFunctor<float> clip_grad_functor;
 launch_multi_tensor_functor<ClipGradNormGroupSize, decltype(clip_grad_functor)>(
   Stream(ctx), ChunkSize, tensor_sizes, grouped_tensor_pointers, clip_grad_functor, total_norm, Epsilon, max_norm_);

 
 TensorSeq* clipped_gradients = ctx->Output<TensorSeq>(0);
 ORT_RETURN_IF_ERROR(PopulateOutput(Stream(ctx), alloc, gradients, &clipped_gradients));

 return Status::OK();
}

} 
} 
